{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ad2898-74e7-40fe-9099-ba44e88d301c",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "'\n",
    "Working on the Italian_Parkinsons_Voice_and_Speech dataset downloaded from here: https://huggingface.co/datasets/birgermoell/Italian_Parkinsons_Voice_and_Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13af6abf-4316-4ee9-a224-9e2ecd479aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 1986\n",
    "from speechbrain.utils.data_utils import get_all_files\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import torchaudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fc4ec-094e-4749-94dc-3d0c037b5d54",
   "metadata": {},
   "source": [
    "## Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccad2d0-5796-4dc8-8d22-67b78d53fff6",
   "metadata": {},
   "source": [
    "|  | # wav files | # speakers |\n",
    "| --- | --- | --- |\n",
    "| Dataset | 831 | 61 |\n",
    "| Train Data | 649 | 48 |\n",
    "| Valid Data | 96 (downsampled to 64 for class balance) | 6 |\n",
    "| Test Data | 86 | 7 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6068f87-49e3-404d-9e1f-8a2e7a633d59",
   "metadata": {},
   "source": [
    "### get audio files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f77cd78e-5978-42af-958f-1de0dd145cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size=  831\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your code here\n",
    "data_files = get_all_files('/home/ulaval.ca/maelr5/scratch/parkinsons', match_and=['.wav'])\n",
    "\n",
    "print('data size= ', len(data_files))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "39c69bf7-7957-4f0e-b797-ddf0dba3cc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " '/home/ulaval.ca/maelr5/scratch/parkinsons/15 Young Healthy Control/Daniele R/B1LBULCAAS94M100120171057.wav',\n",
       " \"/home/ulaval.ca/maelr5/scratch/parkinsons/28 People with Parkinson's disease/17-28/Nicola M/VE1NMIICNOO52M100220171138.wav\")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_files), data_files[0], data_files[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2d00ea55-50a9-4003-ab3f-7fad0f73a80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['',\n",
       "  'home',\n",
       "  'ulaval.ca',\n",
       "  'maelr5',\n",
       "  'scratch',\n",
       "  'parkinsons',\n",
       "  '15 Young Healthy Control',\n",
       "  'Daniele R',\n",
       "  'B1LBULCAAS94M100120171057.wav'],\n",
       " ['',\n",
       "  'home',\n",
       "  'ulaval.ca',\n",
       "  'maelr5',\n",
       "  'scratch',\n",
       "  'parkinsons',\n",
       "  \"28 People with Parkinson's disease\",\n",
       "  '17-28',\n",
       "  'Nicola M',\n",
       "  'VE1NMIICNOO52M100220171138.wav'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files[0].split(os.sep), data_files[500].split(os.sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713c3a0-3d41-4317-a657-23586f561a8b",
   "metadata": {},
   "source": [
    "### extract metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a4c3107-83fd-4847-8f68-6bbd67d89446",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for wav_path in data_files:\n",
    "    parts = wav_path.split(os.sep)\n",
    "\n",
    "    # Assumes folder format: /home/ulaval.ca/maelr5/scratch/parkinsons/<label-folder>/<speaker>/<file.wav>\n",
    "    speaker_id = parts[-2]    # e.g., \"Davide S\"\n",
    "    filename = parts[-1]\n",
    "    \n",
    "    if \"Healthy Control\" in wav_path:\n",
    "        label_folder = parts[-3]  # e.g: \"15 Young Healthy Control\"\n",
    "    elif \"with Parkinson's disease\" in wav_path:\n",
    "        label_folder = parts[-4] # e.g: \"28 People with Parkinson's disease\"\n",
    "\n",
    "    # Determine label from folder name\n",
    "    label = \"HC\" if \"Healthy Control\" in label_folder else \"PD\"\n",
    "\n",
    "    data.append({\n",
    "        \"filename\": filename,\n",
    "        \"full_path\": wav_path,\n",
    "        \"speaker_id\": speaker_id,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8bfb2975-612a-41df-b26f-6a1d0f0c02a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "831\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>full_path</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1LBULCAAS94M100120171057.wav</td>\n",
       "      <td>/home/ulaval.ca/maelr5/scratch/parkinsons/15 Y...</td>\n",
       "      <td>Daniele R</td>\n",
       "      <td>HC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B2LBULCAAS94M100120171057.wav</td>\n",
       "      <td>/home/ulaval.ca/maelr5/scratch/parkinsons/15 Y...</td>\n",
       "      <td>Daniele R</td>\n",
       "      <td>HC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        filename  \\\n",
       "0  B1LBULCAAS94M100120171057.wav   \n",
       "1  B2LBULCAAS94M100120171057.wav   \n",
       "\n",
       "                                           full_path speaker_id label  \n",
       "0  /home/ulaval.ca/maelr5/scratch/parkinsons/15 Y...  Daniele R    HC  \n",
       "1  /home/ulaval.ca/maelr5/scratch/parkinsons/15 Y...  Daniele R    HC  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "51ac4285-db86-4f48-abd8-a3a8905bd250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>full_path</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>VE2lbuairgo52M1606161815.wav</td>\n",
       "      <td>/home/ulaval.ca/maelr5/scratch/parkinsons/28 P...</td>\n",
       "      <td>Luigi B</td>\n",
       "      <td>PD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>FB1lbuairgo52M1606161825.wav</td>\n",
       "      <td>/home/ulaval.ca/maelr5/scratch/parkinsons/28 P...</td>\n",
       "      <td>Luigi B</td>\n",
       "      <td>PD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  \\\n",
       "829  VE2lbuairgo52M1606161815.wav   \n",
       "830  FB1lbuairgo52M1606161825.wav   \n",
       "\n",
       "                                             full_path speaker_id label  \n",
       "829  /home/ulaval.ca/maelr5/scratch/parkinsons/28 P...    Luigi B    PD  \n",
       "830  /home/ulaval.ca/maelr5/scratch/parkinsons/28 P...    Luigi B    PD  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a1dbf5-391c-4c79-ae28-02cead144685",
   "metadata": {},
   "source": [
    "### split data into train/ valid/ test sets **\"by speaker\"**:\n",
    "\n",
    "80% Training, 10%Validation, 10% Test\n",
    "\n",
    "Splitting **by speaker** means train and test must not include recordings from the same person, to get more reliable results and because splitting **by recordings** causes **data leakage**, and models just learn to recognize the person — not Parkinson's symptoms.[1]\n",
    "\n",
    "[1] Iswarya Kannoth Veetil, Sowmya V., Juan Rafael Orozco-Arroyave, E.A. Gopalakrishnan,\n",
    "Robust language independent voice data driven Parkinson’s disease detection,\n",
    "Engineering Applications of Artificial Intelligence,\n",
    "Volume 129,\n",
    "2024,\n",
    "107494,\n",
    "ISSN 0952-1976,\n",
    "https://doi.org/10.1016/j.engappai.2023.107494.\n",
    "(https://www.sciencedirect.com/science/article/pii/S0952197623016780)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e0c079c4-83b3-4cdd-a1dd-8df141452086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Daniele R'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['speaker_id'].unique()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a34c85c7-eec3-4e03-9d46-14b2f2e78356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train wavfiles size=  649\n",
      "valid wavfiles size=  96\n",
      "test wavfiles size=  86\n"
     ]
    }
   ],
   "source": [
    "speakers = df['speaker_id'].unique()\n",
    "train_speakers, eval_speakers = train_test_split(speakers, test_size=0.2, train_size=0.8, random_state=seed_value, shuffle=True)\n",
    "valid_speakers, test_speakers = train_test_split(eval_speakers, test_size=0.5, train_size=0.5, random_state=seed_value, shuffle=True)\n",
    "\n",
    "train_df = df[df['speaker_id'].isin(train_speakers)]\n",
    "\n",
    "valid_df = df[df['speaker_id'].isin(valid_speakers)]\n",
    "\n",
    "test_df = df[df['speaker_id'].isin(test_speakers)]\n",
    "\n",
    "print('train wavfiles size= ', len(train_df))\n",
    "print('valid wavfiles size= ', len(valid_df))\n",
    "print('test wavfiles size= ', len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f314be4b-a070-41b3-a4c4-4b661413434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data speakers size=  61\n",
      "train speakers size=  48\n",
      "valid speakers size=  6\n",
      "test speakers size=  7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('data speakers size= ', len(speakers))\n",
    "print('train speakers size= ', len(train_speakers))\n",
    "print('valid speakers size= ', len(valid_speakers))\n",
    "print('test speakers size= ', len(test_speakers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33596c-6cd4-4580-8835-76cd2fff61b4",
   "metadata": {},
   "source": [
    "#### Note: some speakers have more recordings than others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42572d-b7f8-4c99-af56-770b0a0503c2",
   "metadata": {},
   "source": [
    "### create json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "537d0ce9-5d91-4e07-a1dc-c59a84890420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B1LBULCAAS94M100120171057'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.splitext('B1LBULCAAS94M100120171057.wav')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff6a365a-d06e-4e41-a853-efd802ba3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_df_to_json(df, json_path, shuffle=True, seed=42):\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    data = {}\n",
    "    for _, row in df.iterrows():\n",
    "        utt_id = os.path.splitext(row['filename'])[0]  # unique id\n",
    "        # Getting info\n",
    "        audioinfo = torchaudio.info(row['full_path'])\n",
    "        # Compute the duration in seconds.\n",
    "        # This is the number of samples divided by the sampling frequency\n",
    "        duration = audioinfo.num_frames / audioinfo.sample_rate\n",
    "        \n",
    "        data[utt_id] = {\n",
    "            \"path\": row['full_path'],\n",
    "            \"spk_id\": row['speaker_id'],\n",
    "            \"length\": duration,\n",
    "            \"detection\": row['label']\n",
    "        }\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Write out the JSONs\n",
    "basic_df_to_json(train_df, \"train.json\", shuffle=True, seed=seed_value)\n",
    "basic_df_to_json(valid_df, \"valid.json\", shuffle=True, seed=seed_value)\n",
    "basic_df_to_json(test_df, \"test.json\", shuffle=True, seed=seed_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa9903-3a77-413f-b440-709e59c5055a",
   "metadata": {},
   "source": [
    "### The json files are formatted in the following way:-\n",
    "\n",
    "test.json:\n",
    "```\n",
    "{\n",
    "  \"B1LBULCAAS94M100120171053\": {\n",
    "    path:\t\"/home/ulaval.ca/maelr5/scratch/parkinsons/15 Young Healthy Control/Arianna P/B1LBULCAAS94M100120171053.wav\",\n",
    "    spk_id:\t\"Arianna P\",\n",
    "    length:\t40.69875,\n",
    "    detection:\t\"HC\",\n",
    "  },\n",
    "  \"VO2NPIICEOR42M020420171811\": {\n",
    "    path:\t\"/home/ulaval.ca/maelr5/scratch/parkinsons/22 Elderly Healthy Control/NICOLA P/VO2NPIICEOR42M020420171811.wav\",\n",
    "    spk_id:\t\"NICOLA P\",\n",
    "    length:\t5.6536875,\n",
    "    detection:\t\"HC\",\n",
    "  },\n",
    "  \"D1cdaopmoe67M2605161905\": {\n",
    "    path:\t\"/home/ulaval.ca/maelr5/scratch/parkinsons/28 People with Parkinson's disease/1-5/Domenico C/D1cdaopmoe67M2605161905.wav\",\n",
    "    spk_id:\t\"Domenico C\",\n",
    "    length:\t5.000725623582767,\n",
    "    detection:\t\"PD\",\n",
    "  },\n",
    "....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e82f62b-d43d-4a0c-9f50-1fdc023de672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class statistics for each set\n",
    "\n",
    "def plot_class_balance(inputfile):\n",
    "    # Load the JSON file\n",
    "    with open(inputfile, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract labels (assumes key is \"detection\")\n",
    "    labels = [entry[\"detection\"] for entry in data.values()]\n",
    "    \n",
    "    # Count samples per class\n",
    "    counts = Counter(labels)\n",
    "    \n",
    "    # Print nicely\n",
    "    for label, count in counts.items():\n",
    "        print(f\"{label}: {count} samples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baa5b911-e6e9-455b-a54d-14af0375d41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:-\n",
      "PD: 325 samples\n",
      "HC: 324 samples\n",
      "valid:-\n",
      "PD: 64 samples\n",
      "HC: 32 samples\n",
      "test:-\n",
      "HC: 38 samples\n",
      "PD: 48 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"train:-\")\n",
    "plot_class_balance(\"train.json\")\n",
    "print(\"valid:-\")\n",
    "plot_class_balance(\"valid.json\")\n",
    "print(\"test:-\")\n",
    "plot_class_balance(\"test.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82eea9-d99d-4975-b7f6-23c042343ec0",
   "metadata": {},
   "source": [
    "### Updated df_to_json() with class balancing\n",
    "\n",
    "We need to class balance the valid set by Downsampling the larger class to match the smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8573a6be-1d23-4d43-833d-0c03e3780cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json(df, json_path, shuffle=True, balance_classes=False, seed=42):\n",
    "    if balance_classes:\n",
    "        # Split into PD and HC\n",
    "        pd_df = df[df['label'] == 'PD']\n",
    "        hc_df = df[df['label'] == 'HC']\n",
    "\n",
    "        # Find the smaller class size\n",
    "        min_size = min(len(pd_df), len(hc_df))\n",
    "\n",
    "        # Downsample both classes\n",
    "        pd_df = pd_df.sample(n=min_size, random_state=seed)\n",
    "        hc_df = hc_df.sample(n=min_size, random_state=seed)\n",
    "\n",
    "        # Combine and shuffle\n",
    "        df = pd.concat([pd_df, hc_df], axis=0)\n",
    "    \n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    data = {}\n",
    "    for _, row in df.iterrows():\n",
    "        utt_id = os.path.splitext(row['filename'])[0]  # unique id\n",
    "        # Getting info\n",
    "        audioinfo = torchaudio.info(row['full_path'])\n",
    "        # Compute the duration in seconds.\n",
    "        # This is the number of samples divided by the sampling frequency\n",
    "        duration = audioinfo.num_frames / audioinfo.sample_rate\n",
    "        \n",
    "        data[utt_id] = {\n",
    "            \"path\": row['full_path'],\n",
    "            \"spk_id\": row['speaker_id'],\n",
    "            \"length\": duration,\n",
    "            \"detection\": row['label']\n",
    "        }\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c24e31ab-507e-48d3-86ba-5483cbc5fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_to_json(train_df, \"train.json\", shuffle=True, balance_classes=False, seed=seed_value)\n",
    "df_to_json(valid_df, \"valid.json\", shuffle=True, balance_classes=True, seed=seed_value)\n",
    "df_to_json(test_df, \"test.json\", shuffle=True, balance_classes=False, seed=seed_value)  # usually test is untouched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "496ebd68-b1b7-4ff0-9bab-954b09c14a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:-\n",
      "PD: 325 samples\n",
      "HC: 324 samples\n",
      "valid:-\n",
      "PD: 32 samples\n",
      "HC: 32 samples\n",
      "test:-\n",
      "HC: 38 samples\n",
      "PD: 48 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"train:-\")\n",
    "plot_class_balance(\"train.json\")\n",
    "print(\"valid:-\")\n",
    "plot_class_balance(\"valid.json\")\n",
    "print(\"test:-\")\n",
    "plot_class_balance(\"test.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d6fe5-8274-4e60-88d7-93dd02e4ee6b",
   "metadata": {},
   "source": [
    "#### sanity check : to show that there is not overlap betweeen train and test\n",
    "(1) Compare by Audio Paths (Most Reliable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4ad6145-9a79-40a8-ba5d-c1f3cbd9d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_paths(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return set(ex[\"path\"] for ex in data.values())\n",
    "\n",
    "def load_speakers(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return set(ex[\"spk_id\"] for ex in data.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa2b95bb-d09b-47a2-be50-cae38474a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of overlapping audio files between train and test: 0\n",
      "\n",
      " Number of overlapping audio files between train and valid: 0\n",
      "\n",
      " Number of overlapping audio files between valid and test: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_paths = load_paths(\"train.json\")\n",
    "valid_paths = load_paths(\"valid.json\")\n",
    "test_paths = load_paths(\"test.json\")\n",
    "\n",
    "overlap = train_paths.intersection(test_paths)\n",
    "print(f\"\\n Number of overlapping audio files between train and test: {len(overlap)}\")\n",
    "if overlap:\n",
    "    print(\"Some overlapping files:\")\n",
    "    for p in list(overlap)[:10]:  # show first 10\n",
    "        print(\"-\", p)\n",
    "\n",
    "overlap = train_paths.intersection(valid_paths)\n",
    "print(f\"\\n Number of overlapping audio files between train and valid: {len(overlap)}\")\n",
    "\n",
    "overlap = valid_paths.intersection(test_paths)\n",
    "print(f\"\\n Number of overlapping audio files between valid and test: {len(overlap)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936aa2fc-e325-4dba-be7f-252de3edb8c7",
   "metadata": {},
   "source": [
    "(2) Compare by Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8fd7d28-2f24-4455-8a57-1839d36a4194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎙️ Overlapping speakers between train and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between train and valid: 0\n",
      "\n",
      "🎙️ Overlapping speakers between valid and test: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_speakers = load_speakers(\"train.json\")\n",
    "valid_speakers = load_speakers(\"valid.json\")\n",
    "test_speakers = load_speakers(\"test.json\")\n",
    "\n",
    "overlap_speakers = train_speakers.intersection(test_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between train and test: {len(overlap_speakers)}\")\n",
    "\n",
    "overlap_speakers = train_speakers.intersection(valid_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between train and valid: {len(overlap_speakers)}\")\n",
    "\n",
    "overlap_speakers = valid_speakers.intersection(test_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between valid and test: {len(overlap_speakers)}\")\n",
    "\n",
    "if overlap_speakers:\n",
    "    print(\"Some shared speakers:\")\n",
    "    for s in list(overlap_speakers)[:10]:\n",
    "        print(\"-\", s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b4b4d-8d53-4437-84ca-1f325335a5de",
   "metadata": {},
   "source": [
    "## Create small data for sanity check of the model:\n",
    "\n",
    "The model should overfit this data after training for multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cae78cb-2e1a-4d79-8ad6-99e4360d8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torchaudio\n",
    "\n",
    "def df_to_small_json(df, json_path, shuffle=True, balance_classes=False, seed=42, samples_per_class=None):\n",
    "    if balance_classes:\n",
    "        # Split into PD and HC\n",
    "        pd_df = df[df['label'] == 'PD']\n",
    "        hc_df = df[df['label'] == 'HC']\n",
    "\n",
    "        # Optionally limit samples per class\n",
    "        if samples_per_class:\n",
    "            pd_df = pd_df.sample(n=min(samples_per_class, len(pd_df)), random_state=seed)\n",
    "            hc_df = hc_df.sample(n=min(samples_per_class, len(hc_df)), random_state=seed)\n",
    "        \n",
    "        df = pd.concat([pd_df, hc_df], axis=0)\n",
    "\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    data = {}\n",
    "    for _, row in df.iterrows():\n",
    "        utt_id = os.path.splitext(os.path.basename(row['filename']))[0]\n",
    "        audioinfo = torchaudio.info(row['full_path'])\n",
    "        duration = audioinfo.num_frames / audioinfo.sample_rate\n",
    "\n",
    "        data[utt_id] = {\n",
    "            \"path\": row['full_path'],\n",
    "            \"spk_id\": row['speaker_id'],\n",
    "            \"length\": duration,\n",
    "            \"detection\": row['label']\n",
    "        }\n",
    "\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Keep your full train/valid/test splits\n",
    "# But only use a balanced subset when exporting to JSON\n",
    "\n",
    "df_to_small_json(train_df, \"train-check.json\", shuffle=True, balance_classes=True, samples_per_class=10)\n",
    "df_to_small_json(valid_df, \"valid-check.json\", shuffle=True, balance_classes=True, samples_per_class=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4a89097-2c62-433f-bf6a-8b4ad7d40f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:-\n",
      "PD: 10 samples\n",
      "HC: 10 samples\n",
      "valid:-\n",
      "PD: 2 samples\n",
      "HC: 2 samples\n",
      "test:-\n",
      "HC: 38 samples\n",
      "PD: 48 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"train:-\")\n",
    "plot_class_balance(\"train-check.json\")\n",
    "print(\"valid:-\")\n",
    "plot_class_balance(\"valid-check.json\")\n",
    "print(\"test:-\")\n",
    "plot_class_balance(\"test.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff042abd-776d-411c-8fc5-094db283c417",
   "metadata": {},
   "source": [
    "## k-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516914ed-ca52-4492-9586-98d30e34ec51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "453663e5-9fd7-4bf8-ad81-16c1ff91b1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 649, 96, 1986)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(train_df), len(valid_df), seed_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d3ca127e-4e53-404a-8ed5-b80c92ea11aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Fold 0: Train and Val JSONs\n",
      "Saved Fold 1: Train and Val JSONs\n",
      "Saved Fold 2: Train and Val JSONs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Save a DataFrame to JSON\n",
    "def save_json(df, json_path):\n",
    "    data = {}\n",
    "    for _, row in df.iterrows():\n",
    "        utt_id = os.path.splitext(row['filename'])[0]\n",
    "        audioinfo = torchaudio.info(row['full_path'])\n",
    "        duration = audioinfo.num_frames / audioinfo.sample_rate\n",
    "        data[utt_id] = {\n",
    "            \"path\": row['full_path'],\n",
    "            \"spk_id\": row['speaker_id'],\n",
    "            \"length\": duration,\n",
    "            \"detection\": row['label']\n",
    "        }\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "def balance_classes(df, seed=42):\n",
    "    pd_df = df[df['label'] == 'PD']\n",
    "    hc_df = df[df['label'] == 'HC']\n",
    "    min_size = min(len(pd_df), len(hc_df))\n",
    "    \n",
    "    pd_df = pd_df.sample(n=min_size, random_state=seed)\n",
    "    hc_df = hc_df.sample(n=min_size, random_state=seed)\n",
    "    \n",
    "    return pd.concat([pd_df, hc_df]).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "# Perform K-Fold only on train_df (ignoring test_df)\n",
    "def cross_val_on_train(train_df, k=5, seed=42, balance_train=True):\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "        fold_train = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "        fold_val = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "        if balance_train:\n",
    "            fold_train = balance_classes(fold_train)\n",
    "\n",
    "        save_json(fold_train, f\"fold{fold}_train.json\")\n",
    "        save_json(fold_val, f\"fold{fold}_valid.json\")\n",
    "        print(f\"Saved Fold {fold}: Train and Val JSONs\")\n",
    "\n",
    "\n",
    "full_train_df = pd.concat([train_df, valid_df]).reset_index(drop=True)\n",
    "\n",
    "# If you just want to use train_df only:\n",
    "cross_val_on_train(full_train_df, k=3, seed=seed_value, balance_train=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e4c8d0c2-d9cc-4a14-8639-346fa4409a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold0_train:-\n",
      "HC: 237 samples\n",
      "PD: 237 samples\n",
      "fold0_val:-\n",
      "HC: 119 samples\n",
      "PD: 130 samples\n",
      "fold1_train:-\n",
      "HC: 237 samples\n",
      "PD: 237 samples\n",
      "fold1_val:-\n",
      "HC: 119 samples\n",
      "PD: 129 samples\n",
      "fold2_train:-\n",
      "HC: 238 samples\n",
      "PD: 238 samples\n",
      "fold2_val:-\n",
      "HC: 118 samples\n",
      "PD: 130 samples\n"
     ]
    }
   ],
   "source": [
    "for fold in range(3):\n",
    "    print(f\"fold{fold}_train:-\")\n",
    "    plot_class_balance(f\"fold{fold}_train.json\")\n",
    "    print(f\"fold{fold}_valid:-\")\n",
    "    plot_class_balance(f\"fold{fold}_valid.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d34dc87d-9386-4128-8676-5524e884f976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of overlapping audio files between fold0_train and test: 0\n",
      "\n",
      " Number of overlapping audio files between fold0_train and vfold0_alid: 0\n",
      "\n",
      " Number of overlapping audio files between fold0_valid and test: 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fold0_valid.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Number of overlapping audio files between fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_valid and test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(overlap)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m train_speakers = load_speakers(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_train.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m valid_speakers = \u001b[43mload_speakers\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfold\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_valid.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m test_speakers = load_speakers(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtest.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m overlap_speakers = train_speakers.intersection(test_speakers)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mload_speakers\u001b[39m\u001b[34m(json_path)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_speakers\u001b[39m(json_path):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      8\u001b[39m         data = json.load(f)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(ex[\u001b[33m\"\u001b[39m\u001b[33mspk_id\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m data.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parkinsons/parksenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'fold0_valid.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "for fold in range(3):\n",
    "    train_paths = load_paths(f\"train.json\")\n",
    "    valid_paths = load_paths(f\"valid.json\")\n",
    "    test_paths = load_paths(f\"test.json\")\n",
    "    \n",
    "    overlap = train_paths.intersection(test_paths)\n",
    "    print(f\"\\n Number of overlapping audio files between fold{fold}_train and test: {len(overlap)}\")\n",
    "    if overlap:\n",
    "        print(f\"Some overlapping fold{fold}_files:\")\n",
    "        for p in list(overlap)[:10]:  # show first 10\n",
    "            print(\"-\", p)\n",
    "    \n",
    "    overlap = train_paths.intersection(valid_paths)\n",
    "    print(f\"\\n Number of overlapping audio files between fold{fold}_train and vfold{fold}_alid: {len(overlap)}\")\n",
    "    \n",
    "    overlap = valid_paths.intersection(test_paths)\n",
    "    print(f\"\\n Number of overlapping audio files between fold{fold}_valid and test: {len(overlap)}\")\n",
    "    \n",
    "    \n",
    "    train_speakers = load_speakers(f\"fold{fold}_train.json\")\n",
    "    valid_speakers = load_speakers(f\"fold{fold}_valid.json\")\n",
    "    test_speakers = load_speakers(f\"test.json\")\n",
    "    \n",
    "    overlap_speakers = train_speakers.intersection(test_speakers)\n",
    "    print(f\"\\n🎙️ Overlapping speakers between fold{fold}_train and test: {len(overlap_speakers)}\")\n",
    "    \n",
    "    overlap_speakers = train_speakers.intersection(valid_speakers)\n",
    "    print(f\"\\n🎙️ Overlapping speakers between fold{fold}_train and fold{fold}_fold{fold}_valid: {len(overlap_speakers)}\")\n",
    "    \n",
    "    overlap_speakers = valid_speakers.intersection(test_speakers)\n",
    "    print(f\"\\n🎙️ Overlapping speakers between fold{fold}_valid and test: {len(overlap_speakers)}\")\n",
    "    \n",
    "    if overlap_speakers:\n",
    "        print(f\"Some shared fold{fold}_speakers:\")\n",
    "        for s in list(overlap_speakers)[:10]:\n",
    "            print(\"-\", s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381841a7-2055-4250-885d-fba1fe40ade1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66d168-e0e1-4870-93ee-5e0a7744c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22da115-a3ac-484b-a7fc-5387bbe154f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4213ec4d-03d6-4b10-9d16-52e216a25b3b",
   "metadata": {},
   "source": [
    "### number of examples in each class \n",
    "\n",
    "to check the classes balance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d602a2-bdde-4815-92bf-4723199234c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 young healthy data size=  45\n",
      "22 elderly healthy data size=  349\n",
      "37 candidates - Healthy data size=  394\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.utils.data_utils import get_all_files\n",
    "\n",
    "young_healthy_files = get_all_files(\"/home/ulaval.ca/maelr5/scratch/parkinsons/15 Young Healthy Control\", match_and=['.wav'])\n",
    "elderly_healthy_files = get_all_files(\"/home/ulaval.ca/maelr5/scratch/parkinsons/22 Elderly Healthy Control\", match_and=['.wav'])\n",
    "\n",
    "print('15 young healthy data size= ', len(young_healthy_files))\n",
    "print('22 elderly healthy data size= ', len(elderly_healthy_files))\n",
    "print('37 candidates - Healthy data size= ', len(young_healthy_files) + len(elderly_healthy_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23964c3d-357d-4c32-8a2c-865b4bccd4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 candidates - with Parkinson's disease data size=  437\n"
     ]
    }
   ],
   "source": [
    "data_files = get_all_files(\"/home/ulaval.ca/maelr5/scratch/parkinsons/28 People with Parkinson's disease\", match_and=['.wav'])\n",
    "\n",
    "print(\"28 candidates - with Parkinson's disease data size= \", len(data_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e0184f-3ea9-4fdf-b785-1d253cf164bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 39 Healthy (1) in test\n",
    "# 45 PD (0) in test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7d11a-b35d-4078-bcd5-80c80f90d7dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# decrease the data and use the short recordings of vowels\n",
    "\n",
    "Sustained Vowels: Participants are asked to produce sustained phonations of vowels, such as 'a','e','o','i','u'. These recordings are particularly useful for analyzing fundamental frequency (F0) variations and other acoustic features that can indicate PD-related changes in voice production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db54f9dd-def8-44e1-9b25-e9433bff862e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size=  495\n",
      "train size=  396\n",
      "valid size=  49\n",
      "test size=  50\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from speechbrain.utils.data_utils import get_all_files\n",
    "\n",
    "# Your code here\n",
    "data_files = get_all_files('/home/ulaval.ca/maelr5/scratch/parkinsons',\n",
    "                           match_and=['.wav'],\n",
    "                           match_or=['VA1','VA2','VE1','VE2','VI1','VI2','VO1','VO2','VU1','VU2'],\n",
    "                          )\n",
    "\n",
    "print('data size= ', len(data_files))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_files, test_files = train_test_split(data_files, test_size=0.2, train_size=0.8, random_state=seed_value, shuffle=True)\n",
    "valid_files, test_files = train_test_split(test_files, test_size=0.5, train_size=0.5, random_state=seed_value, shuffle=True)\n",
    "\n",
    "print('train size= ', len(train_files))\n",
    "print('valid size= ', len(valid_files))\n",
    "print('test size= ', len(test_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1374c7b-5160-4028-a971-bdbdb5e38d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torchaudio\n",
    "\n",
    "def create_json(json_file, audiolist):\n",
    "  json_dict = {}\n",
    "  for audiofile in audiolist:\n",
    "\n",
    "    # Getting info\n",
    "    audioinfo = torchaudio.info(audiofile)\n",
    "\n",
    "    # Compute the duration in seconds.\n",
    "    # This is the number of samples divided by the sampling frequency\n",
    "    duration = audioinfo.num_frames / audioinfo.sample_rate\n",
    "\n",
    "    # Get spk Label by manipulating the audio path\n",
    "    if \"Healthy Control\" in audiofile:\n",
    "        detection_id = \"Healthy Control\"\n",
    "    elif \"with Parkinson's disease\" in audiofile:\n",
    "        detection_id = \"with Parkinson's disease\"\n",
    "\n",
    "    # Get a unique utterance id\n",
    "    uttid = audiofile.split('/')[-2] + '_' + audiofile.split('/')[-1][:-4]\n",
    "\n",
    "    # Create entry for this utterance\n",
    "    json_dict[uttid] = {\n",
    "            \"path\": audiofile,\n",
    "            \"length\": duration,\n",
    "            \"detection\": detection_id,\n",
    "    }\n",
    "\n",
    "    # Writing the dictionary to the json file\n",
    "    with open(json_file, mode=\"w\") as json_f:\n",
    "      json.dump(json_dict, json_f, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "create_json('train-vowels.json', train_files)\n",
    "create_json('valid-vowels.json', valid_files)\n",
    "create_json('test-vowels.json', test_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4485b-ff63-4bd4-b70e-faf7c420f1a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# decrease the data and only use the short recordings of vowel 'a'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0bd736b-bc8f-4aa9-a7cd-35e8111e6f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size=  99\n",
      "train size=  79\n",
      "valid size=  10\n",
      "test size=  10\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from speechbrain.utils.data_utils import get_all_files\n",
    "\n",
    "# Your code here\n",
    "data_files = get_all_files('/home/ulaval.ca/maelr5/scratch/parkinsons',\n",
    "                           match_and=['.wav'],\n",
    "                           match_or=['VA1', 'VA2'],\n",
    "                          )\n",
    "\n",
    "print('data size= ', len(data_files))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_files, test_files = train_test_split(data_files, test_size=0.2, train_size=0.8, random_state=seed_value, shuffle=True)\n",
    "valid_files, test_files = train_test_split(test_files, test_size=0.5, train_size=0.5, random_state=seed_value, shuffle=True)\n",
    "\n",
    "print('train size= ', len(train_files))\n",
    "print('valid size= ', len(valid_files))\n",
    "print('test size= ', len(test_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e49efc9-a83e-486e-bcf8-a9a8e1174491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torchaudio\n",
    "\n",
    "def create_json(json_file, audiolist):\n",
    "  json_dict = {}\n",
    "  for audiofile in audiolist:\n",
    "\n",
    "    # Getting info\n",
    "    audioinfo = torchaudio.info(audiofile)\n",
    "\n",
    "    # Compute the duration in seconds.\n",
    "    # This is the number of samples divided by the sampling frequency\n",
    "    duration = audioinfo.num_frames / audioinfo.sample_rate\n",
    "\n",
    "    # Get spk Label by manipulating the audio path\n",
    "    if \"Healthy Control\" in audiofile:\n",
    "        detection_id = \"Healthy Control\"\n",
    "    elif \"with Parkinson's disease\" in audiofile:\n",
    "        detection_id = \"with Parkinson's disease\"\n",
    "\n",
    "    # Get a unique utterance id\n",
    "    uttid = audiofile.split('/')[-2] + '_' + audiofile.split('/')[-1][:-4]\n",
    "\n",
    "    # Create entry for this utterance\n",
    "    json_dict[uttid] = {\n",
    "            \"path\": audiofile,\n",
    "            \"length\": duration,\n",
    "            \"detection\": detection_id,\n",
    "    }\n",
    "\n",
    "    # Writing the dictionary to the json file\n",
    "    with open(json_file, mode=\"w\") as json_f:\n",
    "      json.dump(json_dict, json_f, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "create_json('train-vowel-a.json', train_files)\n",
    "create_json('valid-vowel-a.json', valid_files)\n",
    "create_json('test-vowel-a.json', test_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02430d71-ded8-4384-a489-7f35eb2c4857",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# decrease the data and use the recordings of short sentences or phrases (not vowels)\n",
    "Phrases: Short sentences or phrases are recorded to evaluate more complex speech patterns. \n",
    "These recordings help in assessing prosody, articulation, and other speech characteristics that may be affected by PD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af9904cd-466e-4768-9a86-ad6735dcf42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size=  336\n",
      "train size=  268\n",
      "valid size=  34\n",
      "test size=  34\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from speechbrain.utils.data_utils import get_all_files\n",
    "\n",
    "# Your code here\n",
    "data_files = get_all_files('/home/ulaval.ca/maelr5/scratch/parkinsons',\n",
    "                           match_and=['.wav'],\n",
    "                           exclude_or=['VA1','VA2','VE1','VE2','VI1','VI2','VO1','VO2','VU1','VU2'],\n",
    "                          )\n",
    "\n",
    "print('data size= ', len(data_files))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_files, test_files = train_test_split(data_files, test_size=0.2, train_size=0.8, random_state=seed_value, shuffle=True)\n",
    "valid_files, test_files = train_test_split(test_files, test_size=0.5, train_size=0.5, random_state=seed_value, shuffle=True)\n",
    "\n",
    "print('train size= ', len(train_files))\n",
    "print('valid size= ', len(valid_files))\n",
    "print('test size= ', len(test_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6b07340-1cd5-4083-a2ff-61a57d65d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torchaudio\n",
    "\n",
    "def create_json(json_file, audiolist):\n",
    "  json_dict = {}\n",
    "  for audiofile in audiolist:\n",
    "\n",
    "    # Getting info\n",
    "    audioinfo = torchaudio.info(audiofile)\n",
    "\n",
    "    # Compute the duration in seconds.\n",
    "    # This is the number of samples divided by the sampling frequency\n",
    "    duration = audioinfo.num_frames / audioinfo.sample_rate\n",
    "\n",
    "    # Get spk Label by manipulating the audio path\n",
    "    if \"Healthy Control\" in audiofile:\n",
    "        detection_id = \"Healthy Control\"\n",
    "    elif \"with Parkinson's disease\" in audiofile:\n",
    "        detection_id = \"with Parkinson's disease\"\n",
    "\n",
    "    # Get a unique utterance id\n",
    "    uttid = audiofile.split('/')[-2] + '_' + audiofile.split('/')[-1][:-4]\n",
    "\n",
    "    # Create entry for this utterance\n",
    "    json_dict[uttid] = {\n",
    "            \"path\": audiofile,\n",
    "            \"length\": duration,\n",
    "            \"detection\": detection_id,\n",
    "    }\n",
    "\n",
    "    # Writing the dictionary to the json file\n",
    "    with open(json_file, mode=\"w\") as json_f:\n",
    "      json.dump(json_dict, json_f, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "create_json('train-phrases.json', train_files)\n",
    "create_json('valid-phrases.json', valid_files)\n",
    "create_json('test-phrases.json', test_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ca6b2-231d-440d-a9ba-d8a1b2e76fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parksenv",
   "language": "python",
   "name": "parksenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
