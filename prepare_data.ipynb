{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ad2898-74e7-40fe-9099-ba44e88d301c",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "'\n",
    "Working on the Italian_Parkinsons_Voice_and_Speech dataset downloaded from here: https://huggingface.co/datasets/birgermoell/Italian_Parkinsons_Voice_and_Speech\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225d682-4328-4331-bd90-accf114d822f",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13af6abf-4316-4ee9-a224-9e2ecd479aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 1986\n",
    "from speechbrain.utils.data_utils import get_all_files\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import torchaudio\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "from data_prepare_funs import extract_metadata, split_by_speaker, df_to_json, load_paths, load_speakers, calculate_class_samples, df_to_small_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fc4ec-094e-4749-94dc-3d0c037b5d54",
   "metadata": {},
   "source": [
    "## Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccad2d0-5796-4dc8-8d22-67b78d53fff6",
   "metadata": {},
   "source": [
    "|  | # wav files | # speakers |\n",
    "| --- | --- | --- |\n",
    "| Dataset | 831 | 61 |\n",
    "| Train Data | 649 | 48 |\n",
    "| Valid Data | 96 | 6 |\n",
    "| Test Data | 86 | 7 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6068f87-49e3-404d-9e1f-8a2e7a633d59",
   "metadata": {},
   "source": [
    "### get audio files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77cd78e-5978-42af-958f-1de0dd145cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size=  831\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your code here\n",
    "data_files = get_all_files('/home/ulaval.ca/maelr5/scratch/parkinsons', match_and=['.wav'])\n",
    "\n",
    "print('data size= ', len(data_files))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c69bf7-7957-4f0e-b797-ddf0dba3cc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " '/home/ulaval.ca/maelr5/scratch/parkinsons/15 Young Healthy Control/Daniele R/B1LBULCAAS94M100120171057.wav',\n",
       " \"/home/ulaval.ca/maelr5/scratch/parkinsons/28 People with Parkinson's disease/17-28/Nicola M/VE1NMIICNOO52M100220171138.wav\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_files), data_files[0], data_files[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d00ea55-50a9-4003-ab3f-7fad0f73a80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['',\n",
       "  'home',\n",
       "  'ulaval.ca',\n",
       "  'maelr5',\n",
       "  'scratch',\n",
       "  'parkinsons',\n",
       "  '15 Young Healthy Control',\n",
       "  'Daniele R',\n",
       "  'B1LBULCAAS94M100120171057.wav'],\n",
       " ['',\n",
       "  'home',\n",
       "  'ulaval.ca',\n",
       "  'maelr5',\n",
       "  'scratch',\n",
       "  'parkinsons',\n",
       "  \"28 People with Parkinson's disease\",\n",
       "  '17-28',\n",
       "  'Nicola M',\n",
       "  'VE1NMIICNOO52M100220171138.wav'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files[0].split(os.sep), data_files[500].split(os.sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713c3a0-3d41-4317-a657-23586f561a8b",
   "metadata": {},
   "source": [
    "### extract metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4c3107-83fd-4847-8f68-6bbd67d89446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = extract_metadata(data_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bfb2975-612a-41df-b26f-6a1d0f0c02a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "831\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>full_path</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1LBULCAAS94M100120171057.wav</td>\n",
       "      <td>/home/ulaval.ca/maelr5/scratch/parkinsons/15 Y...</td>\n",
       "      <td>Daniele R</td>\n",
       "      <td>HC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B2LBULCAAS94M100120171057.wav</td>\n",
       "      <td>/home/ulaval.ca/maelr5/scratch/parkinsons/15 Y...</td>\n",
       "      <td>Daniele R</td>\n",
       "      <td>HC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        filename  \\\n",
       "0  B1LBULCAAS94M100120171057.wav   \n",
       "1  B2LBULCAAS94M100120171057.wav   \n",
       "\n",
       "                                           full_path speaker_id label  \n",
       "0  /home/ulaval.ca/maelr5/scratch/parkinsons/15 Y...  Daniele R    HC  \n",
       "1  /home/ulaval.ca/maelr5/scratch/parkinsons/15 Y...  Daniele R    HC  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ac4285-db86-4f48-abd8-a3a8905bd250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>full_path</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>VE2lbuairgo52M1606161815.wav</td>\n",
       "      <td>/home/ulaval.ca/maelr5/scratch/parkinsons/28 P...</td>\n",
       "      <td>Luigi B</td>\n",
       "      <td>PD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>FB1lbuairgo52M1606161825.wav</td>\n",
       "      <td>/home/ulaval.ca/maelr5/scratch/parkinsons/28 P...</td>\n",
       "      <td>Luigi B</td>\n",
       "      <td>PD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  \\\n",
       "829  VE2lbuairgo52M1606161815.wav   \n",
       "830  FB1lbuairgo52M1606161825.wav   \n",
       "\n",
       "                                             full_path speaker_id label  \n",
       "829  /home/ulaval.ca/maelr5/scratch/parkinsons/28 P...    Luigi B    PD  \n",
       "830  /home/ulaval.ca/maelr5/scratch/parkinsons/28 P...    Luigi B    PD  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a1dbf5-391c-4c79-ae28-02cead144685",
   "metadata": {},
   "source": [
    "### split data into train/ valid/ test sets **\"by speaker\"**:\n",
    "\n",
    "80% Training, 10%Validation, 10% Test\n",
    "\n",
    "Splitting **by speaker** means train and test must not include recordings from the same person, to get more reliable results and because splitting **by recordings** causes **data leakage**, and models just learn to recognize the person — not Parkinson's symptoms.[1]\n",
    "\n",
    "[1] Iswarya Kannoth Veetil, Sowmya V., Juan Rafael Orozco-Arroyave, E.A. Gopalakrishnan,\n",
    "Robust language independent voice data driven Parkinson’s disease detection,\n",
    "Engineering Applications of Artificial Intelligence,\n",
    "Volume 129,\n",
    "2024,\n",
    "107494,\n",
    "ISSN 0952-1976,\n",
    "https://doi.org/10.1016/j.engappai.2023.107494.\n",
    "(https://www.sciencedirect.com/science/article/pii/S0952197623016780)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0c079c4-83b3-4cdd-a1dd-8df141452086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Daniele R'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['speaker_id'].unique()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a34c85c7-eec3-4e03-9d46-14b2f2e78356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data speakers size=  61\n",
      "train speakers size=  48\n",
      "valid speakers size=  6\n",
      "test speakers size=  7\n",
      "*****************************************\n",
      "train wavfiles size=  649\n",
      "valid wavfiles size=  96\n",
      "test wavfiles size=  86\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df, valid_df, test_df = split_by_speaker(df)\n",
    "print('*****************************************')\n",
    "print('train wavfiles size= ', len(train_df))\n",
    "print('valid wavfiles size= ', len(valid_df))\n",
    "print('test wavfiles size= ', len(test_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33596c-6cd4-4580-8835-76cd2fff61b4",
   "metadata": {},
   "source": [
    "#### Note: some speakers have more recordings than others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82eea9-d99d-4975-b7f6-23c042343ec0",
   "metadata": {},
   "source": [
    "### create json files\n",
    "\n",
    "The class balancing is done by Downsampling the majority class to match the smaller one, to reduce the risk of overfitting the repeated samples from the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "537d0ce9-5d91-4e07-a1dc-c59a84890420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B1LBULCAAS94M100120171057'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.splitext('B1LBULCAAS94M100120171057.wav')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c782aae-9e92-4379-a9e5-893e110510fb",
   "metadata": {},
   "source": [
    "### auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c24e31ab-507e-48d3-86ba-5483cbc5fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_to_json(train_df, \"train.json\", shuffle=True, balance_classes=False, seed=seed_value)\n",
    "df_to_json(valid_df, \"valid.json\", shuffle=True, balance_classes=False, seed=seed_value)\n",
    "df_to_json(test_df, \"test.json\", shuffle=True, balance_classes=False, seed=seed_value)  # usually test and valid is untouched\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa9903-3a77-413f-b440-709e59c5055a",
   "metadata": {},
   "source": [
    "#### The json files are formatted in the following way:-\n",
    "\n",
    "test.json:\n",
    "```\n",
    "{\n",
    "  \"VA1GCIALSDA52F170320171127\": {\n",
    "        path:\t\"/home/ulaval.ca/maelr5/scratch/parkinsons/22 Elderly Healthy Control/GILDA C/VA1GCIALSDA52F170320171127.wav\"\n",
    "        spk_id:\t\"GILDA C\"\n",
    "        length:\t11.15625\n",
    "        detection:\t\"HC\"\n",
    "  },\n",
    "  \"VO1cdaopmoe67M2605161911\": {\n",
    "        path:\t\"/home/ulaval.ca/maelr5/scratch/parkinsons/28 People with Parkinson's disease/1-5/Domenico C/VO1cdaopmoe67M2605161911.wav\"\n",
    "        spk_id:\t\"Domenico C\"\n",
    "        length:\t18.500725623582767\n",
    "        detection:\t\"PD\"\n",
    "  },\n",
    "....\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1dd991-9acf-4dfb-a0d5-d3210fcc0a13",
   "metadata": {},
   "source": [
    "#### check class statistics for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "496ebd68-b1b7-4ff0-9bab-954b09c14a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:-\n",
      "PD: 325 samples\n",
      "HC: 324 samples\n",
      "valid:-\n",
      "PD: 64 samples\n",
      "HC: 32 samples\n",
      "test:-\n",
      "HC: 38 samples\n",
      "PD: 48 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"train:-\")\n",
    "calculate_class_samples(\"train.json\")\n",
    "print(\"valid:-\")\n",
    "calculate_class_samples(\"valid.json\")\n",
    "print(\"test:-\")\n",
    "calculate_class_samples(\"test.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774acb1-cc96-439c-842a-b3321c57440a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "777eb947-444d-42d9-94be-bd06b5f14512",
   "metadata": {},
   "source": [
    "#### Reasons for Not doing class balancing for the data:\n",
    "\n",
    "- The train data is already balanced.\n",
    "- The validation and test sets are fixed to reflect how the model would perform on real-world imbalanced data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d6fe5-8274-4e60-88d7-93dd02e4ee6b",
   "metadata": {},
   "source": [
    "#### sanity check : to show that there is not overlap betweeen train and test samples\n",
    "(1) Compare by Audio Paths (Most Reliable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa2b95bb-d09b-47a2-be50-cae38474a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of overlapping audio files between train and test: 0\n",
      "\n",
      " Number of overlapping audio files between train and valid: 0\n",
      "\n",
      " Number of overlapping audio files between valid and test: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_paths = load_paths(\"train.json\")\n",
    "valid_paths = load_paths(\"valid.json\")\n",
    "test_paths = load_paths(\"test.json\")\n",
    "\n",
    "overlap = train_paths.intersection(test_paths)\n",
    "print(f\"\\n Number of overlapping audio files between train and test: {len(overlap)}\")\n",
    "if overlap:\n",
    "    print(\"Some overlapping files:\")\n",
    "    for p in list(overlap)[:10]:  # show first 10\n",
    "        print(\"-\", p)\n",
    "\n",
    "overlap = train_paths.intersection(valid_paths)\n",
    "print(f\"\\n Number of overlapping audio files between train and valid: {len(overlap)}\")\n",
    "\n",
    "overlap = valid_paths.intersection(test_paths)\n",
    "print(f\"\\n Number of overlapping audio files between valid and test: {len(overlap)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936aa2fc-e325-4dba-be7f-252de3edb8c7",
   "metadata": {},
   "source": [
    "(2) Compare by Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8fd7d28-2f24-4455-8a57-1839d36a4194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎙️ Overlapping speakers between train and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between train and valid: 0\n",
      "\n",
      "🎙️ Overlapping speakers between valid and test: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_speakers = load_speakers(\"train.json\")\n",
    "valid_speakers = load_speakers(\"valid.json\")\n",
    "test_speakers = load_speakers(\"test.json\")\n",
    "\n",
    "overlap_speakers = train_speakers.intersection(test_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between train and test: {len(overlap_speakers)}\")\n",
    "\n",
    "overlap_speakers = train_speakers.intersection(valid_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between train and valid: {len(overlap_speakers)}\")\n",
    "\n",
    "overlap_speakers = valid_speakers.intersection(test_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between valid and test: {len(overlap_speakers)}\")\n",
    "\n",
    "if overlap_speakers:\n",
    "    print(\"Some shared speakers:\")\n",
    "    for s in list(overlap_speakers)[:10]:\n",
    "        print(\"-\", s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b4b4d-8d53-4437-84ca-1f325335a5de",
   "metadata": {},
   "source": [
    "## Create small data for sanity check of the model:\n",
    "\n",
    "The model should overfit this data after training for multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cae78cb-2e1a-4d79-8ad6-99e4360d8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torchaudio\n",
    "\n",
    "df_to_small_json(train_df, \"train-check.json\", shuffle=True, balance_classes=True, samples_per_class=10)\n",
    "df_to_small_json(valid_df, \"valid-check.json\", shuffle=True, balance_classes=True, samples_per_class=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4a89097-2c62-433f-bf6a-8b4ad7d40f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:-\n",
      "PD: 10 samples\n",
      "HC: 10 samples\n",
      "valid:-\n",
      "PD: 2 samples\n",
      "HC: 2 samples\n",
      "test:-\n",
      "HC: 38 samples\n",
      "PD: 48 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"train:-\")\n",
    "calculate_class_samples(\"train-check.json\")\n",
    "print(\"valid:-\")\n",
    "calculate_class_samples(\"valid-check.json\")\n",
    "print(\"test:-\")\n",
    "calculate_class_samples(\"test.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff042abd-776d-411c-8fc5-094db283c417",
   "metadata": {},
   "source": [
    "## k-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516914ed-ca52-4492-9586-98d30e34ec51",
   "metadata": {},
   "source": [
    "Cross-Validation Procedure: Stratified k-fold cross-validation (with k = 5) is employed to evaluate model performance while preserving class distribution across folds. The dataset was split into three stratified folds, ensuring that each fold maintained the original proportion of Parkinson's Disease (PD) and Healthy Control (HC) samples. For each iteration, the model was trained on four folds and validated on the remaining one. To mitigate class imbalance during training, class balancing is applied by randomly downsampling the majority class within the training set of each fold. The validation sets remained unbalanced to reflect the natural class distribution and to provide a realistic assessment of model performance. The same predefined test set was used for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "453663e5-9fd7-4bf8-ad81-16c1ff91b1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 649, 96, 1986)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(train_df), len(valid_df), seed_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3ca127e-4e53-404a-8ed5-b80c92ea11aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Train Speakers = 43, Val Speakers = 11...\n",
      "Fold 1: Train Speakers = 43, Val Speakers = 11...\n",
      "Fold 2: Train Speakers = 43, Val Speakers = 11...\n",
      "Fold 3: Train Speakers = 43, Val Speakers = 11...\n",
      "Fold 4: Train Speakers = 44, Val Speakers = 10...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Save a DataFrame to JSON\n",
    "def save_json(df, json_path):\n",
    "    data = {}\n",
    "    for _, row in df.iterrows():\n",
    "        utt_id = os.path.splitext(row['filename'])[0]\n",
    "        audioinfo = torchaudio.info(row['full_path'])\n",
    "        duration = audioinfo.num_frames / audioinfo.sample_rate\n",
    "        data[utt_id] = {\n",
    "            \"path\": row['full_path'],\n",
    "            \"spk_id\": row['speaker_id'],\n",
    "            \"length\": duration,\n",
    "            \"detection\": row['label']\n",
    "        }\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "def balance_classes(df, seed=42):\n",
    "    pd_df = df[df['label'] == 'PD']\n",
    "    hc_df = df[df['label'] == 'HC']\n",
    "    min_size = min(len(pd_df), len(hc_df))\n",
    "    \n",
    "    pd_df = pd_df.sample(n=min_size, random_state=seed)\n",
    "    hc_df = hc_df.sample(n=min_size, random_state=seed)\n",
    "    \n",
    "    return pd.concat([pd_df, hc_df]).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "# Perform K-Fold only on train_df (ignoring test_df)\n",
    "def cross_val_on_train(train_df, k=5, seed=42, balance_train=True):\n",
    "    # Group by speaker\n",
    "    speaker_df = train_df.groupby(\"speaker_id\").first().reset_index()[[\"speaker_id\", \"label\"]]\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # split by speaker\n",
    "    for fold, (train_spk_idx, val_spk_idx) in enumerate(skf.split(speaker_df, speaker_df['label'])):\n",
    "        train_speakers = speaker_df.iloc[train_spk_idx]['speaker_id']\n",
    "        val_speakers = speaker_df.iloc[val_spk_idx]['speaker_id']\n",
    "        \n",
    "        fold_train = train_df[train_df['speaker_id'].isin(train_speakers)].reset_index(drop=True)\n",
    "        fold_val = train_df[train_df['speaker_id'].isin(val_speakers)].reset_index(drop=True)\n",
    "        if balance_train:\n",
    "            fold_train = balance_classes(fold_train)\n",
    "\n",
    "        save_json(fold_train, f\"fold{fold}_train.json\")\n",
    "        save_json(fold_val, f\"fold{fold}_valid.json\")\n",
    "        print(f\"Fold {fold}: Train Speakers = {len(train_speakers)}, Val Speakers = {len(val_speakers)}...\")\n",
    "\n",
    "# Combine train + valid for cross-val\n",
    "full_train_df = pd.concat([train_df, valid_df]).reset_index(drop=True)\n",
    "\n",
    "# If you just want to use train_df only:\n",
    "cross_val_on_train(full_train_df, k=5, seed=seed_value, balance_train=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4c8d0c2-d9cc-4a14-8639-346fa4409a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold0_train:-\n",
      "PD: 270 samples\n",
      "HC: 270 samples\n",
      "fold0_valid:-\n",
      "HC: 86 samples\n",
      "PD: 80 samples\n",
      "fold1_train:-\n",
      "HC: 296 samples\n",
      "PD: 296 samples\n",
      "fold1_valid:-\n",
      "HC: 60 samples\n",
      "PD: 88 samples\n",
      "fold2_train:-\n",
      "PD: 309 samples\n",
      "HC: 309 samples\n",
      "fold2_valid:-\n",
      "HC: 47 samples\n",
      "PD: 61 samples\n",
      "fold3_train:-\n",
      "PD: 289 samples\n",
      "HC: 289 samples\n",
      "fold3_valid:-\n",
      "HC: 67 samples\n",
      "PD: 80 samples\n",
      "fold4_train:-\n",
      "HC: 260 samples\n",
      "PD: 260 samples\n",
      "fold4_valid:-\n",
      "HC: 96 samples\n",
      "PD: 80 samples\n"
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "    print(f\"fold{fold}_train:-\")\n",
    "    calculate_class_samples(f\"fold{fold}_train.json\")\n",
    "    print(f\"fold{fold}_valid:-\")\n",
    "    calculate_class_samples(f\"fold{fold}_valid.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d34dc87d-9386-4128-8676-5524e884f976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of overlapping audio files between fold0_train and test: 0\n",
      "\n",
      " Number of overlapping audio files between fold0_train and fold0_valid: 0\n",
      "\n",
      " Number of overlapping audio files between fold0_valid and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold0_train and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold0_train and fold0_valid: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold0_valid and test: 0\n",
      "\n",
      " Number of overlapping audio files between fold1_train and test: 0\n",
      "\n",
      " Number of overlapping audio files between fold1_train and fold1_valid: 0\n",
      "\n",
      " Number of overlapping audio files between fold1_valid and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold1_train and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold1_train and fold1_valid: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold1_valid and test: 0\n",
      "\n",
      " Number of overlapping audio files between fold2_train and test: 0\n",
      "\n",
      " Number of overlapping audio files between fold2_train and fold2_valid: 0\n",
      "\n",
      " Number of overlapping audio files between fold2_valid and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold2_train and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold2_train and fold2_valid: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold2_valid and test: 0\n",
      "\n",
      " Number of overlapping audio files between fold3_train and test: 0\n",
      "\n",
      " Number of overlapping audio files between fold3_train and fold3_valid: 0\n",
      "\n",
      " Number of overlapping audio files between fold3_valid and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold3_train and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold3_train and fold3_valid: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold3_valid and test: 0\n",
      "\n",
      " Number of overlapping audio files between fold4_train and test: 0\n",
      "\n",
      " Number of overlapping audio files between fold4_train and fold4_valid: 0\n",
      "\n",
      " Number of overlapping audio files between fold4_valid and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold4_train and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold4_train and fold4_valid: 0\n",
      "\n",
      "🎙️ Overlapping speakers between fold4_valid and test: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for fold in range(5):\n",
    "    train_paths = load_paths(f\"fold{fold}_train.json\")\n",
    "    valid_paths = load_paths(f\"fold{fold}_valid.json\")\n",
    "    test_paths = load_paths(f\"test.json\")\n",
    "    \n",
    "    overlap = train_paths.intersection(test_paths)\n",
    "    print(f\"\\n Number of overlapping audio files between fold{fold}_train and test: {len(overlap)}\")\n",
    "    if overlap:\n",
    "        print(f\"Some overlapping fold{fold}_files:\")\n",
    "        for p in list(overlap)[:10]:  # show first 10\n",
    "            print(\"-\", p)\n",
    "    \n",
    "    overlap = train_paths.intersection(valid_paths)\n",
    "    print(f\"\\n Number of overlapping audio files between fold{fold}_train and fold{fold}_valid: {len(overlap)}\")\n",
    "    \n",
    "    overlap = valid_paths.intersection(test_paths)\n",
    "    print(f\"\\n Number of overlapping audio files between fold{fold}_valid and test: {len(overlap)}\")\n",
    "    \n",
    "    \n",
    "    train_speakers = load_speakers(f\"fold{fold}_train.json\")\n",
    "    valid_speakers = load_speakers(f\"fold{fold}_valid.json\")\n",
    "    test_speakers = load_speakers(f\"test.json\")\n",
    "    \n",
    "    overlap_speakers = train_speakers.intersection(test_speakers)\n",
    "    print(f\"\\n🎙️ Overlapping speakers between fold{fold}_train and test: {len(overlap_speakers)}\")\n",
    "    \n",
    "    overlap_speakers = train_speakers.intersection(valid_speakers)\n",
    "    print(f\"\\n🎙️ Overlapping speakers between fold{fold}_train and fold{fold}_valid: {len(overlap_speakers)}\")\n",
    "    \n",
    "    overlap_speakers = valid_speakers.intersection(test_speakers)\n",
    "    print(f\"\\n🎙️ Overlapping speakers between fold{fold}_valid and test: {len(overlap_speakers)}\")\n",
    "    \n",
    "    if overlap_speakers:\n",
    "        print(f\"Some shared fold{fold}_speakers:\")\n",
    "        for s in list(overlap_speakers)[:10]:\n",
    "            print(\"-\", s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381841a7-2055-4250-885d-fba1fe40ade1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66d168-e0e1-4870-93ee-5e0a7744c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22da115-a3ac-484b-a7fc-5387bbe154f1",
   "metadata": {},
   "source": [
    "## Data statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4213ec4d-03d6-4b10-9d16-52e216a25b3b",
   "metadata": {},
   "source": [
    "### number of examples in each class \n",
    "\n",
    "to check the classes balance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d602a2-bdde-4815-92bf-4723199234c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 young healthy data size=  45\n",
      "22 elderly healthy data size=  349\n",
      "37 candidates - Healthy data size=  394\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.utils.data_utils import get_all_files\n",
    "\n",
    "young_healthy_files = get_all_files(\"/home/ulaval.ca/maelr5/scratch/parkinsons/15 Young Healthy Control\", match_and=['.wav'])\n",
    "elderly_healthy_files = get_all_files(\"/home/ulaval.ca/maelr5/scratch/parkinsons/22 Elderly Healthy Control\", match_and=['.wav'])\n",
    "\n",
    "print('15 young healthy data size= ', len(young_healthy_files))\n",
    "print('22 elderly healthy data size= ', len(elderly_healthy_files))\n",
    "print('37 candidates - Healthy data size= ', len(young_healthy_files) + len(elderly_healthy_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23964c3d-357d-4c32-8a2c-865b4bccd4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 candidates - with Parkinson's disease data size=  437\n"
     ]
    }
   ],
   "source": [
    "data_files = get_all_files(\"/home/ulaval.ca/maelr5/scratch/parkinsons/28 People with Parkinson's disease\", match_and=['.wav'])\n",
    "\n",
    "print(\"28 candidates - with Parkinson's disease data size= \", len(data_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e0184f-3ea9-4fdf-b785-1d253cf164bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 39 Healthy (1) in test\n",
    "# 45 PD (0) in test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7d11a-b35d-4078-bcd5-80c80f90d7dd",
   "metadata": {},
   "source": [
    "# decrease the data and use the short recordings of vowels\n",
    "\n",
    "Sustained Vowels: Participants are asked to produce sustained phonations of vowels, such as 'a','e','o','i','u'. These recordings are particularly useful for analyzing fundamental frequency (F0) variations and other acoustic features that can indicate PD-related changes in voice production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db54f9dd-def8-44e1-9b25-e9433bff862e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size=  495\n",
      "495\n",
      "data speakers size=  46\n",
      "train speakers size=  36\n",
      "valid speakers size=  5\n",
      "test speakers size=  5\n",
      "*****************************************\n",
      "train wavfiles size=  385\n",
      "valid wavfiles size=  60\n",
      "test wavfiles size=  50\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from speechbrain.utils.data_utils import get_all_files\n",
    "\n",
    "# Your code here\n",
    "data_files = get_all_files('/home/ulaval.ca/maelr5/scratch/parkinsons',\n",
    "                           match_and=['.wav'],\n",
    "                           match_or=['VA1','VA2','VE1','VE2','VI1','VI2','VO1','VO2','VU1','VU2'],\n",
    "                          )\n",
    "\n",
    "print('data size= ', len(data_files))\n",
    "\n",
    "vowels_df = extract_metadata(data_files)\n",
    "print(len(vowels_df))\n",
    "# print(vowels_df.head(2))\n",
    "\n",
    "vowels_train_df, vowels_valid_df, vowels_test_df = split_by_speaker(vowels_df, seed_value = 900)\n",
    "print('*****************************************')\n",
    "print('train wavfiles size= ', len(vowels_train_df))\n",
    "print('valid wavfiles size= ', len(vowels_valid_df))\n",
    "print('test wavfiles size= ', len(vowels_test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1374c7b-5160-4028-a971-bdbdb5e38d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_json(vowels_train_df, \"train_vowels.json\", shuffle=True, balance_classes=True, seed=seed_value)\n",
    "df_to_json(vowels_valid_df, \"valid_vowels.json\", shuffle=True, balance_classes=False, seed=seed_value)\n",
    "df_to_json(vowels_test_df, \"test_vowels.json\", shuffle=True, balance_classes=False, seed=seed_value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ecbfffd-2f18-4db3-8170-c7eec8573684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:-\n",
      "HC: 170 samples\n",
      "PD: 170 samples\n",
      "valid:-\n",
      "PD: 30 samples\n",
      "HC: 30 samples\n",
      "test:-\n",
      "HC: 20 samples\n",
      "PD: 30 samples\n",
      "\n",
      " Number of overlapping audio files between train and test: 0\n",
      "\n",
      " Number of overlapping audio files between train and valid: 0\n",
      "\n",
      " Number of overlapping audio files between valid and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between train and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between train and valid: 0\n",
      "\n",
      "🎙️ Overlapping speakers between valid and test: 0\n"
     ]
    }
   ],
   "source": [
    "vowels_train_json = \"train_vowels.json\"\n",
    "vowels_valid_json = \"valid_vowels.json\"\n",
    "vowels_test_json = \"test_vowels.json\"\n",
    "\n",
    "print(\"train:-\")\n",
    "calculate_class_samples(vowels_train_json)\n",
    "print(\"valid:-\")\n",
    "calculate_class_samples(vowels_valid_json)\n",
    "print(\"test:-\")\n",
    "calculate_class_samples(vowels_test_json)\n",
    "\n",
    "\n",
    "train_paths = load_paths(vowels_train_json)\n",
    "valid_paths = load_paths(vowels_valid_json)\n",
    "test_paths = load_paths(vowels_test_json)\n",
    "\n",
    "overlap = train_paths.intersection(test_paths)\n",
    "print(f\"\\n Number of overlapping audio files between train and test: {len(overlap)}\")\n",
    "if overlap:\n",
    "    print(\"Some overlapping files:\")\n",
    "    for p in list(overlap)[:10]:  # show first 10\n",
    "        print(\"-\", p)\n",
    "\n",
    "overlap = train_paths.intersection(valid_paths)\n",
    "print(f\"\\n Number of overlapping audio files between train and valid: {len(overlap)}\")\n",
    "\n",
    "overlap = valid_paths.intersection(test_paths)\n",
    "print(f\"\\n Number of overlapping audio files between valid and test: {len(overlap)}\")\n",
    "\n",
    "\n",
    "\n",
    "train_speakers = load_speakers(vowels_train_json)\n",
    "valid_speakers = load_speakers(vowels_valid_json)\n",
    "test_speakers = load_speakers(vowels_test_json)\n",
    "\n",
    "overlap_speakers = train_speakers.intersection(test_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between train and test: {len(overlap_speakers)}\")\n",
    "\n",
    "overlap_speakers = train_speakers.intersection(valid_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between train and valid: {len(overlap_speakers)}\")\n",
    "\n",
    "overlap_speakers = valid_speakers.intersection(test_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between valid and test: {len(overlap_speakers)}\")\n",
    "\n",
    "if overlap_speakers:\n",
    "    print(\"Some shared speakers:\")\n",
    "    for s in list(overlap_speakers)[:10]:\n",
    "        print(\"-\", s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f2f51-8f3f-49f2-ac46-9d3ea91274c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02430d71-ded8-4384-a489-7f35eb2c4857",
   "metadata": {},
   "source": [
    "# decrease the data and use the recordings of short sentences or phrases (not vowels)\n",
    "Phrases: Short sentences or phrases are recorded to evaluate more complex speech patterns. \n",
    "These recordings help in assessing prosody, articulation, and other speech characteristics that may be affected by PD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af9904cd-466e-4768-9a86-ad6735dcf42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrases data size=  336\n",
      "336\n",
      "data speakers size=  61\n",
      "train speakers size=  48\n",
      "valid speakers size=  6\n",
      "test speakers size=  7\n",
      "*****************************************\n",
      "train wavfiles size=  265\n",
      "valid wavfiles size=  38\n",
      "test wavfiles size=  33\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from speechbrain.utils.data_utils import get_all_files\n",
    "\n",
    "# Your code here\n",
    "data_files = get_all_files('/home/ulaval.ca/maelr5/scratch/parkinsons',\n",
    "                           match_and=['.wav'],\n",
    "                           exclude_or=['VA1','VA2','VE1','VE2','VI1','VI2','VO1','VO2','VU1','VU2'],\n",
    "                          )\n",
    "\n",
    "print('phrases data size= ', len(data_files))\n",
    "\n",
    "phrases_df = extract_metadata(data_files)\n",
    "print(len(phrases_df))\n",
    "\n",
    "phrases_train_df, phrases_valid_df, phrases_test_df = split_by_speaker(phrases_df, seed_value = 900)\n",
    "print('*****************************************')\n",
    "print('train wavfiles size= ', len(phrases_train_df))\n",
    "print('valid wavfiles size= ', len(phrases_valid_df))\n",
    "print('test wavfiles size= ', len(phrases_test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6b07340-1cd5-4083-a2ff-61a57d65d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_json(phrases_train_df, \"train_phrases.json\", shuffle=True, balance_classes=True, seed=seed_value)\n",
    "df_to_json(phrases_valid_df, \"valid_phrases.json\", shuffle=True, balance_classes=False, seed=seed_value)\n",
    "df_to_json(phrases_test_df, \"test_phrases.json\", shuffle=True, balance_classes=False, seed=seed_value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e9ca6b2-231d-440d-a9ba-d8a1b2e76fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:-\n",
      "HC: 121 samples\n",
      "PD: 121 samples\n",
      "valid:-\n",
      "HC: 15 samples\n",
      "PD: 23 samples\n",
      "test:-\n",
      "PD: 18 samples\n",
      "HC: 15 samples\n",
      "\n",
      " Number of overlapping audio files between train and test: 0\n",
      "\n",
      " Number of overlapping audio files between train and valid: 0\n",
      "\n",
      " Number of overlapping audio files between valid and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between train and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between train and valid: 0\n",
      "\n",
      "🎙️ Overlapping speakers between valid and test: 0\n"
     ]
    }
   ],
   "source": [
    "phrases_train_json = \"train_phrases.json\"\n",
    "phrases_valid_json = \"valid_phrases.json\"\n",
    "phrases_test_json = \"test_phrases.json\"\n",
    "\n",
    "print(\"train:-\")\n",
    "calculate_class_samples(phrases_train_json)\n",
    "print(\"valid:-\")\n",
    "calculate_class_samples(phrases_valid_json)\n",
    "print(\"test:-\")\n",
    "calculate_class_samples(phrases_test_json)\n",
    "\n",
    "\n",
    "train_paths = load_paths(phrases_train_json)\n",
    "valid_paths = load_paths(phrases_valid_json)\n",
    "test_paths = load_paths(phrases_test_json)\n",
    "\n",
    "overlap = train_paths.intersection(test_paths)\n",
    "print(f\"\\n Number of overlapping audio files between train and test: {len(overlap)}\")\n",
    "if overlap:\n",
    "    print(\"Some overlapping files:\")\n",
    "    for p in list(overlap)[:10]:  # show first 10\n",
    "        print(\"-\", p)\n",
    "\n",
    "overlap = train_paths.intersection(valid_paths)\n",
    "print(f\"\\n Number of overlapping audio files between train and valid: {len(overlap)}\")\n",
    "\n",
    "overlap = valid_paths.intersection(test_paths)\n",
    "print(f\"\\n Number of overlapping audio files between valid and test: {len(overlap)}\")\n",
    "\n",
    "\n",
    "\n",
    "train_speakers = load_speakers(phrases_train_json)\n",
    "valid_speakers = load_speakers(phrases_valid_json)\n",
    "test_speakers = load_speakers(phrases_test_json)\n",
    "\n",
    "overlap_speakers = train_speakers.intersection(test_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between train and test: {len(overlap_speakers)}\")\n",
    "\n",
    "overlap_speakers = train_speakers.intersection(valid_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between train and valid: {len(overlap_speakers)}\")\n",
    "\n",
    "overlap_speakers = valid_speakers.intersection(test_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between valid and test: {len(overlap_speakers)}\")\n",
    "\n",
    "if overlap_speakers:\n",
    "    print(\"Some shared speakers:\")\n",
    "    for s in list(overlap_speakers)[:10]:\n",
    "        print(\"-\", s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4485b-ff63-4bd4-b70e-faf7c420f1a6",
   "metadata": {},
   "source": [
    "# decrease the data and only use the short recordings of vowel 'a'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0bd736b-bc8f-4aa9-a7cd-35e8111e6f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vowel-a data size=  99\n",
      "99\n",
      "data speakers size=  46\n",
      "train speakers size=  36\n",
      "valid speakers size=  5\n",
      "test speakers size=  5\n",
      "*****************************************\n",
      "train wavfiles size=  77\n",
      "valid wavfiles size=  12\n",
      "test wavfiles size=  10\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from speechbrain.utils.data_utils import get_all_files\n",
    "\n",
    "# Your code here\n",
    "data_files = get_all_files('/home/ulaval.ca/maelr5/scratch/parkinsons',\n",
    "                           match_and=['.wav'],\n",
    "                           match_or=['VA1', 'VA2'],\n",
    "                          )\n",
    "\n",
    "print('vowel-a data size= ', len(data_files))\n",
    "\n",
    "vowela_df = extract_metadata(data_files)\n",
    "print(len(vowela_df))\n",
    "\n",
    "vowela_train_df, vowela_valid_df, vowela_test_df = split_by_speaker(vowela_df, seed_value = 900)\n",
    "print('*****************************************')\n",
    "print('train wavfiles size= ', len(vowela_train_df))\n",
    "print('valid wavfiles size= ', len(vowela_valid_df))\n",
    "print('test wavfiles size= ', len(vowela_test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e49efc9-a83e-486e-bcf8-a9a8e1174491",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_json(vowela_train_df, \"train_vowela.json\", shuffle=True, balance_classes=True, seed=seed_value)\n",
    "df_to_json(vowela_valid_df, \"valid_vowela.json\", shuffle=True, balance_classes=False, seed=seed_value)\n",
    "df_to_json(vowela_test_df, \"test_vowela.json\", shuffle=True, balance_classes=False, seed=seed_value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca9eddd9-08b9-44de-8850-463be5f161f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:-\n",
      "PD: 34 samples\n",
      "HC: 34 samples\n",
      "valid:-\n",
      "PD: 6 samples\n",
      "HC: 6 samples\n",
      "test:-\n",
      "HC: 4 samples\n",
      "PD: 6 samples\n",
      "\n",
      " Number of overlapping audio files between train and test: 0\n",
      "\n",
      " Number of overlapping audio files between train and valid: 0\n",
      "\n",
      " Number of overlapping audio files between valid and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between train and test: 0\n",
      "\n",
      "🎙️ Overlapping speakers between train and valid: 0\n",
      "\n",
      "🎙️ Overlapping speakers between valid and test: 0\n"
     ]
    }
   ],
   "source": [
    "vowela_train_json = \"train_vowela.json\"\n",
    "vowela_valid_json = \"valid_vowela.json\"\n",
    "vowela_test_json = \"test_vowela.json\"\n",
    "\n",
    "print(\"train:-\")\n",
    "calculate_class_samples(vowela_train_json)\n",
    "print(\"valid:-\")\n",
    "calculate_class_samples(vowela_valid_json)\n",
    "print(\"test:-\")\n",
    "calculate_class_samples(vowela_test_json)\n",
    "\n",
    "\n",
    "train_paths = load_paths(vowela_train_json)\n",
    "valid_paths = load_paths(vowela_valid_json)\n",
    "test_paths = load_paths(vowela_test_json)\n",
    "\n",
    "overlap = train_paths.intersection(test_paths)\n",
    "print(f\"\\n Number of overlapping audio files between train and test: {len(overlap)}\")\n",
    "if overlap:\n",
    "    print(\"Some overlapping files:\")\n",
    "    for p in list(overlap)[:10]:  # show first 10\n",
    "        print(\"-\", p)\n",
    "\n",
    "overlap = train_paths.intersection(valid_paths)\n",
    "print(f\"\\n Number of overlapping audio files between train and valid: {len(overlap)}\")\n",
    "\n",
    "overlap = valid_paths.intersection(test_paths)\n",
    "print(f\"\\n Number of overlapping audio files between valid and test: {len(overlap)}\")\n",
    "\n",
    "\n",
    "\n",
    "train_speakers = load_speakers(vowela_train_json)\n",
    "valid_speakers = load_speakers(vowela_valid_json)\n",
    "test_speakers = load_speakers(vowela_test_json)\n",
    "\n",
    "overlap_speakers = train_speakers.intersection(test_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between train and test: {len(overlap_speakers)}\")\n",
    "\n",
    "overlap_speakers = train_speakers.intersection(valid_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between train and valid: {len(overlap_speakers)}\")\n",
    "\n",
    "overlap_speakers = valid_speakers.intersection(test_speakers)\n",
    "print(f\"\\n🎙️ Overlapping speakers between valid and test: {len(overlap_speakers)}\")\n",
    "\n",
    "if overlap_speakers:\n",
    "    print(\"Some shared speakers:\")\n",
    "    for s in list(overlap_speakers)[:10]:\n",
    "        print(\"-\", s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7329b72e-b87d-4554-8f4d-4c12ce11d391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parksenv",
   "language": "python",
   "name": "parksenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
