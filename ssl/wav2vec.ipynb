{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369f205-d0dc-4735-957b-637b52daeb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91396aca-8ed4-49ee-a06c-3e2bf408b687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f0023c1-06d7-4c7d-af2c-3031b0be0c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hparams_wav2vec.yaml\n"
     ]
    }
   ],
   "source": [
    "%%file hparams_wav2vec.yaml\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
    "seed: 1986\n",
    "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
    "\n",
    "# Dataset will be downloaded to the `data_original`\n",
    "data_folder: !ref /home/ulaval.ca/maelr5/scratch/parkinsons\n",
    "output_folder: !ref /home/ulaval.ca/maelr5/scratch/parkinsons-results/train_with_wav2vec2/base/<seed>\n",
    "save_folder: !ref <output_folder>/save\n",
    "train_log: !ref <output_folder>/train_log.txt\n",
    "\n",
    "# URL for the ssl model, you can change to benchmark diffrenet models\n",
    "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
    "# This allow you to have ~4% improvment\n",
    "sslmodel_hub: facebook/wav2vec2-base\n",
    "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
    "\n",
    "# Path where data manifest files will be stored\n",
    "train_annotation: /home/ulaval.ca/maelr5/parkinsons/train.json\n",
    "valid_annotation: /home/ulaval.ca/maelr5/parkinsons/valid.json\n",
    "test_annotation: /home/ulaval.ca/maelr5/parkinsons/test.json\n",
    "\n",
    "# The train logger writes training statistics to a file, as well as stdout.\n",
    "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
    "    save_file: !ref <train_log>\n",
    "\n",
    "# Tensorboard logs\n",
    "use_tensorboard: False\n",
    "tensorboard_logs_folder: !ref <output_folder>/tb_logs/\n",
    "\n",
    "####################### Training Parameters ####################################\n",
    "number_of_epochs: 25\n",
    "batch_size: 1\n",
    "lr: 0.0001\n",
    "lr_ssl: 0.00001\n",
    "\n",
    "#freeze all ssl\n",
    "freeze_ssl: False\n",
    "#set to true to freeze the CONV part of the ssl model\n",
    "# We see an improvement of 2% with freezing CNNs\n",
    "freeze_ssl_conv: True\n",
    "\n",
    "####################### Model Parameters #######################################\n",
    "encoder_dim: 768\n",
    "\n",
    "# Number of emotions\n",
    "out_n_neurons: 2 # (healthy, parkinsons)\n",
    "\n",
    "dataloader_options:\n",
    "    batch_size: !ref <batch_size>\n",
    "    shuffle: True\n",
    "    num_workers: 2  # 2 on linux but 0 works on windows\n",
    "    drop_last: False\n",
    "\n",
    "# ssl encoder\n",
    "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2\n",
    "    source: !ref <sslmodel_hub>\n",
    "    output_norm: True\n",
    "    freeze: !ref <freeze_ssl>\n",
    "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
    "    save_path: !ref <sslmodel_folder>\n",
    "\n",
    "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
    "    return_std: False\n",
    "\n",
    "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
    "    input_size: !ref <encoder_dim>\n",
    "    n_neurons: !ref <out_n_neurons>\n",
    "    bias: False\n",
    "\n",
    "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
    "    limit: !ref <number_of_epochs>\n",
    "\n",
    "modules:\n",
    "    ssl_model: !ref <ssl_model>\n",
    "    output_mlp: !ref <output_mlp>\n",
    "\n",
    "model: !new:torch.nn.ModuleList\n",
    "    - [!ref <output_mlp>]\n",
    "\n",
    "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
    "    apply_log: True\n",
    "\n",
    "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
    "\n",
    "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
    "    metric: !name:speechbrain.nnet.losses.classification_error\n",
    "        reduction: batch\n",
    "\n",
    "opt_class: !name:torch.optim.Adam\n",
    "    lr: !ref <lr>\n",
    "\n",
    "ssl_opt_class: !name:torch.optim.Adam\n",
    "    lr: !ref <lr_ssl>\n",
    "\n",
    "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
    "    initial_value: !ref <lr>\n",
    "    improvement_threshold: 0.0025\n",
    "    annealing_factor: 0.9\n",
    "    patient: 0\n",
    "\n",
    "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
    "    initial_value: !ref <lr_ssl>\n",
    "    improvement_threshold: 0.0025\n",
    "    annealing_factor: 0.9\n",
    "\n",
    "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
    "    checkpoints_dir: !ref <save_folder>\n",
    "    recoverables:\n",
    "        model: !ref <model>\n",
    "        ssl_model: !ref <ssl_model>\n",
    "        lr_annealing_output: !ref <lr_annealing>\n",
    "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
    "        counter: !ref <epoch_counter>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d7dd730-ae5b-4de0-87cc-3e11a3e3db5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainwav2vec.py\n"
     ]
    }
   ],
   "source": [
    "%%file trainwav2vec.py\n",
    "\n",
    "# Your code here\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Recipe for training an emotion recognition system from speech data only using IEMOCAP.\n",
    "The system classifies 4 emotions ( anger, happiness, sadness, neutrality) with a SSL model.\n",
    "\n",
    "To run this recipe, do the following:\n",
    "> python train.py hparams/train.yaml --data_folder /path/to/IEMOCAP_full_release\n",
    "\n",
    "Authors\n",
    " * Yingzhi WANG 2021\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from confusion_matrix_fig import create_cm_fig\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DetectorBrain(sb.Brain):\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Computation pipeline based on a encoder + emotion classifier.\n",
    "        \"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, lens = batch.sig\n",
    "\n",
    "        outputs = self.modules.ssl_model(wavs, lens)\n",
    "\n",
    "        # last dim will be used for AdaptativeAVG pool\n",
    "        outputs = self.hparams.avg_pool(outputs, lens)\n",
    "        outputs = outputs.view(outputs.shape[0], -1)\n",
    "\n",
    "        outputs = self.modules.output_mlp(outputs)\n",
    "        outputs = self.hparams.log_softmax(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "\n",
    "        \"\"\"Computes the loss using speaker-id as label.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # loss = self.hparams.compute_cost(predictions, lang_id)\n",
    "        # if stage != sb.Stage.TRAIN:\n",
    "        #     self.error_metrics.append(batch.id, predictions, lang_id)\n",
    "        _, lens = batch.sig\n",
    "        detection_id, _ = batch.detection_id_encoded\n",
    "\n",
    "        \"\"\"to meet the input form of nll loss\"\"\"\n",
    "        detection_id = detection_id.squeeze(1)\n",
    "        print('predictions shape= ', predictions.shape)\n",
    "        print('detection_id shape= ', detection_id.shape)\n",
    "        # Compute the cost function\n",
    "        loss = self.hparams.compute_cost(predictions, detection_id)\n",
    "\n",
    "        # Append this batch of losses to the loss metric for easy\n",
    "        self.loss_metric.append(\n",
    "            batch.id, predictions, detection_id, reduction=\"batch\"\n",
    "        )\n",
    "\n",
    "        # Compute classification error at test time\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics.append(batch.id, predictions, detection_id)\n",
    "\n",
    "        # Confusion matrices\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            y_true = detection_id.cpu().detach().numpy()#.squeeze(-1)\n",
    "            y_pred = predictions.cpu().detach().numpy().argmax(-1)#.squeeze(-1)\n",
    "        if stage == sb.Stage.TEST:\n",
    "            print('test y_true= ', y_true)\n",
    "            print('test y_pred= ', y_pred)\n",
    "            confusion_matix = confusion_matrix(\n",
    "                y_true,\n",
    "                y_pred,\n",
    "                labels=sorted(self.hparams.label_encoder.ind2lab.keys()),\n",
    "            )\n",
    "            self.test_confusion_matrix += confusion_matix\n",
    "\n",
    "        # Compute accuracy using MetricStats\n",
    "        print('prediction', predictions)\n",
    "        print('detection_id', detection_id)\n",
    "        self.acc_metric.append(\n",
    "            batch.id, predict=predictions, target=detection_id, lengths=lens,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_stage_start(self, stage, epoch=None):\n",
    "        \"\"\"Gets called at the beginning of each epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set up statistics trackers for this stage\n",
    "        self.loss_metric = sb.utils.metric_stats.MetricStats(\n",
    "            metric=sb.nnet.losses.nll_loss\n",
    "        )\n",
    "\n",
    "        # Compute accuracy using MetricStats\n",
    "        # Define function taking (prediction, target, length) for eval\n",
    "        def accuracy_value(predict, target, lengths):\n",
    "            \"\"\"Computes accuracy.\"\"\"\n",
    "            print(\"Predictions shape:\", predict.shape)\n",
    "            print(\"Detection ID shape:\", target.shape)\n",
    "            print(\"Lengths shape:\", lengths.shape)\n",
    "            \n",
    "            predict = predict.unsqueeze(1)\n",
    "            target = target.unsqueeze(1)\n",
    "            print(\"Predictions after argmax shape:\", predict.shape)\n",
    "            print(\"Detection ID target shape:\", target.shape)\n",
    "\n",
    "            nbr_correct, nbr_total = sb.utils.Accuracy.Accuracy(\n",
    "                predict, target, lengths\n",
    "            )\n",
    "            acc = torch.tensor([nbr_correct / nbr_total])\n",
    "            return acc\n",
    "\n",
    "        self.acc_metric = sb.utils.metric_stats.MetricStats(\n",
    "            metric=accuracy_value, n_jobs=1\n",
    "        )\n",
    "        if stage == sb.Stage.TEST:\n",
    "            self.test_confusion_matrix = np.zeros(\n",
    "                shape=(self.hparams.out_n_neurons, self.hparams.out_n_neurons),\n",
    "                dtype=int,\n",
    "            )\n",
    "\n",
    "        # Set up evaluation-only statistics trackers\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics = self.hparams.error_stats()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
    "        \"\"\"Gets called at the end of an epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
    "        stage_loss : float\n",
    "            The average loss for all of the data processed in this stage.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the train loss until the validation stage.\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_loss = stage_loss\n",
    "            self.train_stats = {\n",
    "                \"loss\": self.train_loss,\n",
    "                \"acc\": self.acc_metric.summarize(\"average\"),\n",
    "            }\n",
    "\n",
    "        # Summarize the statistics from the stage for record-keeping.\n",
    "        elif stage == sb.Stage.VALID:\n",
    "            valid_stats = {\n",
    "                \"loss\": stage_loss,\n",
    "                \"acc\": self.acc_metric.summarize(\"average\"),\n",
    "                \"error_rate\": self.error_metrics.summarize(\"average\"),\n",
    "            }\n",
    "        # Summarize Test statistics from the stage for record-keeping\n",
    "        else:\n",
    "            test_stats = {\n",
    "                \"loss\": stage_loss,\n",
    "                \"acc\": self.acc_metric.summarize(\"average\"),\n",
    "                \"error_rate\": self.error_metrics.summarize(\"average\"),\n",
    "            }\n",
    "\n",
    "        # At the end of validation...\n",
    "        if stage == sb.Stage.VALID:\n",
    "\n",
    "            old_lr, new_lr = self.hparams.lr_annealing(valid_stats[\"error_rate\"])\n",
    "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
    "\n",
    "            (\n",
    "                old_lr_ssl,\n",
    "                new_lr_ssl,\n",
    "            ) = self.hparams.lr_annealing_ssl(valid_stats[\"error_rate\"])\n",
    "            sb.nnet.schedulers.update_learning_rate(\n",
    "                self.ssl_optimizer, new_lr_ssl\n",
    "            )\n",
    "\n",
    "            # Tensorboard logging\n",
    "            if self.hparams.use_tensorboard:\n",
    "                self.hparams.tensorboard_train_logger.log_stats(\n",
    "                    stats_meta={\"Epoch\": epoch},\n",
    "                    train_stats=self.train_stats,\n",
    "                    valid_stats=valid_stats,\n",
    "                )\n",
    "                \n",
    "            # The train_logger writes a summary to stdout and to the logfile.\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\"Epoch\": epoch, \"lr\": old_lr, \"ssl_lr\": old_lr_ssl},\n",
    "                train_stats={\"loss\": self.train_loss},\n",
    "                valid_stats=valid_stats,\n",
    "            )\n",
    "\n",
    "            # Save the current checkpoint and delete previous checkpoints,\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta=valid_stats, min_keys=[\"error_rate\"]\n",
    "            )  \n",
    "\n",
    "        # We also write statistics about test data to stdout and to the logfile.\n",
    "        if stage == sb.Stage.TEST:\n",
    "            # Per class accuracy from Test confusion matrix\n",
    "            per_class_acc_arr = np.diag(self.test_confusion_matrix) / np.sum(\n",
    "                self.test_confusion_matrix, axis=1\n",
    "            )\n",
    "            per_class_acc_arr_str = \"\\n\" + \"\\n\".join(\n",
    "                \"{:}: {:.3f}\".format(class_id, class_acc)\n",
    "                for class_id, class_acc in enumerate(per_class_acc_arr)\n",
    "            )\n",
    "\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\n",
    "                    \"Epoch loaded\": self.hparams.epoch_counter.current,\n",
    "                    \"\\n Per Class Accuracy\": per_class_acc_arr_str,\n",
    "                    \"\\n Confusion Matrix\": \"\\n{:}\\n\".format(\n",
    "                        self.test_confusion_matrix\n",
    "                    ),\n",
    "                },\n",
    "                test_stats=test_stats,\n",
    "            )\n",
    "\n",
    "    def init_optimizers(self):\n",
    "        \"Initializes the ssl optimizer and model optimizer\"\n",
    "        self.ssl_optimizer = self.hparams.ssl_opt_class(\n",
    "            self.modules.ssl_model.parameters()\n",
    "        )\n",
    "        self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
    "\n",
    "        if self.checkpointer is not None:\n",
    "            self.checkpointer.add_recoverable(\n",
    "                \"ssl_opt\", self.ssl_optimizer\n",
    "            )\n",
    "            self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
    "\n",
    "        self.optimizers_dict = {\n",
    "            \"model_optimizer\": self.optimizer,\n",
    "            \"ssl_optimizer\": self.ssl_optimizer,\n",
    "        }\n",
    "\n",
    "\n",
    "def dataio_prep(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined\n",
    "    functions. We expect `prepare_mini_librispeech` to have been called before\n",
    "    this, so that the `train.json`, `valid.json`,  and `valid.json` manifest\n",
    "    files are available.\n",
    "    Arguments\n",
    "    ---------\n",
    "    hparams : dict\n",
    "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
    "        all the hyperparameters needed for dataset construction and loading.\n",
    "    Returns\n",
    "    -------\n",
    "    datasets : dict\n",
    "        Contains two keys, \"train\" and \"valid\" that correspond\n",
    "        to the appropriate DynamicItemDataset object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define audio pipeline\n",
    "    MAX_SEQ_LEN = 16000\n",
    "    @sb.utils.data_pipeline.takes(\"path\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(wav):\n",
    "        \"\"\"Load the signal, and pass it and its length to the corruption class.\n",
    "        This is done on the CPU in the `collate_fn`.\"\"\"\n",
    "        sig = sb.dataio.dataio.read_audio(wav)\n",
    "        print('%%%%%%%%%%%%%signal length= ', len(sig))\n",
    "        return sig[:MAX_SEQ_LEN]\n",
    "\n",
    "    # Initialization of the label encoder. The label encoder assignes to each\n",
    "    # of the observed label a unique index (e.g, 'spk01': 0, 'spk02': 1, ..)\n",
    "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
    "\n",
    "    # Define label pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"detection\")\n",
    "    @sb.utils.data_pipeline.provides(\"detection\", \"detection_id_encoded\")\n",
    "    def label_pipeline(detection_id):\n",
    "        yield detection_id\n",
    "        detection_id_encoded = label_encoder.encode_label_torch(detection_id)\n",
    "        yield detection_id_encoded\n",
    "\n",
    "    # Define datasets. We also connect the dataset with the data processing\n",
    "    # functions defined above.\n",
    "    datasets = {}\n",
    "    data_info = {\n",
    "        \"train\": hparams[\"train_annotation\"],\n",
    "        \"valid\": hparams[\"valid_annotation\"],\n",
    "        \"test\": hparams[\"test_annotation\"],\n",
    "    }\n",
    "    for dataset in data_info:\n",
    "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "            json_path=data_info[dataset],\n",
    "            replacements={\"data_root\": hparams[\"data_folder\"]},\n",
    "            dynamic_items=[audio_pipeline, label_pipeline],\n",
    "            output_keys=[\"id\", \"sig\", \"detection_id_encoded\"],\n",
    "        )\n",
    "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
    "    # Please, take a look into the lab_enc_file to see the label to index\n",
    "    # mappinng.\n",
    "\n",
    "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
    "    label_encoder.load_or_create(\n",
    "        path=lab_enc_file,\n",
    "        from_didatasets=[datasets[\"train\"]],\n",
    "        output_key=\"detection\",\n",
    "    )\n",
    "\n",
    "    return datasets, label_encoder\n",
    "\n",
    "\n",
    "# RECIPE BEGINS!\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Reading command line arguments.\n",
    "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
    "\n",
    "    # Initialize ddp (useful only for multi-GPU DDP training).\n",
    "    # sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "    # Load hyperparameters file with command-line overrides.\n",
    "    with open(hparams_file) as fin:\n",
    "        hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "    # Create experiment directory\n",
    "    sb.create_experiment_directory(\n",
    "        experiment_directory=hparams[\"output_folder\"],\n",
    "        hyperparams_to_save=hparams_file,\n",
    "        overrides=overrides,\n",
    "    )\n",
    "\n",
    "    # Tensorboard logging\n",
    "    if hparams[\"use_tensorboard\"]:\n",
    "        from speechbrain.utils.train_logger import TensorboardLogger\n",
    "\n",
    "        hparams[\"tensorboard_train_logger\"] = TensorboardLogger(\n",
    "            hparams[\"tensorboard_logs_folder\"]\n",
    "        )\n",
    "\n",
    "    # Create dataset objects \"train\", \"valid\", and \"test\".\n",
    "    datasets, label_encoder = dataio_prep(hparams)\n",
    "\n",
    "    hparams[\"label_encoder\"] = label_encoder\n",
    "    class_labels = sorted(list(label_encoder.ind2lab.values()))\n",
    "    print(\"Class Labels:\", class_labels, list(label_encoder.lab2ind.values()))\n",
    "\n",
    "    hparams[\"ssl_model\"] = hparams[\"ssl_model\"].to(device=run_opts[\"device\"])\n",
    "    # freeze the feature extractor part when unfreezing\n",
    "    if not hparams[\"freeze_ssl\"] and hparams[\"freeze_ssl_conv\"]:\n",
    "        hparams[\"ssl_model\"].model.feature_extractor._freeze_parameters()\n",
    "\n",
    "    # Initialize the Brain object to prepare for mask training.\n",
    "    detection_brain = DetectorBrain(\n",
    "        modules=hparams[\"modules\"],\n",
    "        opt_class=hparams[\"opt_class\"],\n",
    "        hparams=hparams,\n",
    "        run_opts=run_opts,\n",
    "        checkpointer=hparams[\"checkpointer\"],\n",
    "    )\n",
    "\n",
    "    # The `fit()` method iterates the training loop, calling the methods\n",
    "    # necessary to update the parameters of the model. Since all objects\n",
    "    # with changing state are managed by the Checkpointer, training can be\n",
    "    # stopped at any point, and will be resumed on next call.\n",
    "    detection_brain.fit(\n",
    "        epoch_counter=detection_brain.hparams.epoch_counter,\n",
    "        train_set=datasets[\"train\"],\n",
    "        valid_set=datasets[\"valid\"],\n",
    "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    )\n",
    "\n",
    "    # Load the best checkpoint for evaluation\n",
    "    test_stats = detection_brain.evaluate(\n",
    "        test_set=datasets[\"test\"],\n",
    "        min_key=\"error_rate\",\n",
    "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6442e6e-3e83-49e1-9ab6-2f8e757d8d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:20:51.223713: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-04 18:20:51.244475: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-04 18:20:51.251649: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-04 18:20:51.268492: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-04 18:20:53.614759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "config.json: 100%|█████████████████████████| 1.84k/1.84k [00:00<00:00, 13.8MB/s]\n",
      "/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/transformers/configuration_utils.py:315: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "pytorch_model.bin: 100%|██████████████████████| 380M/380M [00:01<00:00, 271MB/s]\n",
      "preprocessor_config.json: 100%|████████████████| 159/159 [00:00<00:00, 1.35MB/s]\n",
      "speechbrain.lobes.models.huggingface_transformers.wav2vec2 - wav2vec 2.0 feature extractor is frozen.\n",
      "speechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "speechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: /home/ulaval.ca/maelr5/scratch/parkinsons-results/train_with_wav2vec2/base/1986\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "Class Labels: ['Healthy Control', \"with Parkinson's disease\"] [0, 1]\n",
      "speechbrain.core - Gradscaler enabled: `False`\n",
      "speechbrain.core - Using training precision: `--precision=fp32`\n",
      "speechbrain.core - Using evaluation precision: `--eval_precision=fp32`\n",
      "speechbrain.core - DetectorBrain Model Statistics:\n",
      "* Total Number of Trainable Parameters: 90.2M\n",
      "* Total Number of Parameters: 94.4M\n",
      "* Trainable Parameters represent 95.5491% of the total size.\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "speechbrain.utils.epoch_loop - Going into epoch 1\n",
      "  0%|                                                   | 0/664 [00:00<?, ?it/s]%%%%%%%%%%%%%signal length=  619082\n",
      "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
      "  0%|                                                   | 0/664 [00:00<?, ?it/s]%%%%%%%%%%%%%signal length=  3057000\n",
      "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
      "  0%|                                                   | 0/664 [00:00<?, ?it/s]%%%%%%%%%%%%%signal length=  112000\n",
      "%%%%%%%%%%%%%signal length=  108849\n",
      "%%%%%%%%%%%%%signal length=  694230\n",
      "  0%|                                                   | 0/664 [00:00<?, ?it/s]\n",
      "speechbrain.core - Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/ssl/trainwav2vec.py\", line 375, in <module>\n",
      "    detection_brain.fit(\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/speechbrain/core.py\", line 1575, in fit\n",
      "    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/speechbrain/core.py\", line 1400, in _fit_train\n",
      "    loss = self.fit_batch(batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/speechbrain/core.py\", line 1199, in fit_batch\n",
      "    outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/ssl/trainwav2vec.py\", line 32, in compute_forward\n",
      "    outputs = self.modules.ssl_model(wavs, lens)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/speechbrain/lobes/models/huggingface_transformers/wav2vec2.py\", line 158, in forward\n",
      "    return self.extract_features(wav, wav_lens)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/speechbrain/lobes/models/huggingface_transformers/wav2vec2.py\", line 182, in extract_features\n",
      "    out = self.model(\n",
      "          ^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 1808, in forward\n",
      "    extract_features = self.feature_extractor(input_values)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 463, in forward\n",
      "    hidden_states = conv_layer(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 360, in forward\n",
      "    hidden_states = self.conv(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 375, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 370, in _conv_forward\n",
      "    return F.conv1d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 6.19 MiB is free. Process 4104541 has 10.64 GiB memory in use. Process 4159374 has 2.03 GiB memory in use. Process 4174570 has 2.02 GiB memory in use. Process 4182587 has 336.00 MiB memory in use. Including non-PyTorch memory, this process has 764.00 MiB memory in use. Of the allocated memory 360.39 MiB is allocated by PyTorch, and 39.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "model.safetensors: 100%|██████████████████████| 380M/380M [00:01<00:00, 314MB/s]\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /home/ulaval.ca/maelr5/scratch/parkinsons-results/train_with_wav2vec2/base/1986\n",
    "\n",
    "import sys\n",
    "\n",
    "!{sys.executable} trainwav2vec.py hparams_wav2vec.yaml --data_folder='/home/ulaval.ca/maelr5/scratch/parkinsons' --device='cuda:0' --number_of_epochs=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f77f72-5020-4352-a182-656a7e41a216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parksenv",
   "language": "python",
   "name": "parksenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
