{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51386075-628a-4594-9166-a17589a766de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c7587-cd28-44fa-b2e3-17562b824ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5df99f2b-99de-48cd-aefc-1d6a26efb2aa",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5122e0-0d64-4b95-94f1-b3af3453d557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 02:09:00.329650: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-22 02:09:00.351302: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-22 02:09:00.358364: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-22 02:09:00.377593: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-22 02:09:03.838624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from plot_tensorboard import plot_training\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import sys\n",
    "from plot_conf_mat import plot_cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c20e50-2473-40d0-a06c-2f519edaa5e1",
   "metadata": {},
   "source": [
    "### haprams file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ba2582-311c-418d-b5d5-1a300e070b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hparams_wavlm.yaml\n"
     ]
    }
   ],
   "source": [
    "%%file hparams_wavlm.yaml\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
    "seed: 1986\n",
    "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
    "\n",
    "# Dataset will be downloaded to the `data_original`\n",
    "data_folder: !ref /home/ulaval.ca/maelr5/scratch/parkinsons\n",
    "output_folder: !ref /home/ulaval.ca/maelr5/scratch/parkinsons-results/wavlm/base/fulldataset/<seed>\n",
    "save_folder: !ref <output_folder>/save\n",
    "train_log: !ref <output_folder>/train_log.txt\n",
    "\n",
    "# URL for the ssl model, you can change to benchmark diffrenet models\n",
    "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
    "# This allow you to have ~4% improvment\n",
    "sslmodel_hub: microsoft/wavlm-base\n",
    "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
    "\n",
    "# Path where data manifest files will be stored\n",
    "train_annotation: /home/ulaval.ca/maelr5/parkinsons/train.json\n",
    "valid_annotation: /home/ulaval.ca/maelr5/parkinsons/valid.json\n",
    "test_annotation: /home/ulaval.ca/maelr5/parkinsons/test.json\n",
    "\n",
    "# The train logger writes training statistics to a file, as well as stdout.\n",
    "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
    "    save_file: !ref <train_log>\n",
    "\n",
    "# Tensorboard logs\n",
    "use_tensorboard: False\n",
    "tensorboard_logs_folder: !ref <output_folder>/tb_logs/\n",
    "\n",
    "####################### Training Parameters ####################################\n",
    "number_of_epochs: 10\n",
    "batch_size: 1\n",
    "lr: 0.0001\n",
    "lr_ssl: 0.00001\n",
    "sample_rate: 16000\n",
    "\n",
    "#freeze all ssl\n",
    "freeze_ssl: False\n",
    "#set to true to freeze the CONV part of the ssl model\n",
    "# We see an improvement of 2% with freezing CNNs\n",
    "freeze_ssl_conv: True\n",
    "\n",
    "####################### Model Parameters #######################################\n",
    "encoder_dim: 768\n",
    "\n",
    "# Number of emotions\n",
    "out_n_neurons: 2 # (healthy, parkinsons)\n",
    "\n",
    "dataloader_options:\n",
    "    batch_size: !ref <batch_size>\n",
    "    shuffle: True\n",
    "    num_workers: 2  # 2 on linux but 0 works on windows\n",
    "    drop_last: False\n",
    "\n",
    "# ssl encoder; WavLM, HuBERT and Wav2Vec2 both use the same SpeechBrain wrapper\n",
    "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2\n",
    "    source: !ref <sslmodel_hub>\n",
    "    output_norm: True\n",
    "    freeze: !ref <freeze_ssl>\n",
    "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
    "    save_path: !ref <sslmodel_folder>\n",
    "\n",
    "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
    "    return_std: False\n",
    "\n",
    "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
    "    input_size: !ref <encoder_dim>\n",
    "    n_neurons: !ref <out_n_neurons>\n",
    "    bias: False\n",
    "\n",
    "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
    "    limit: !ref <number_of_epochs>\n",
    "\n",
    "modules:\n",
    "    ssl_model: !ref <ssl_model>\n",
    "    output_mlp: !ref <output_mlp>\n",
    "\n",
    "model: !new:torch.nn.ModuleList\n",
    "    - [!ref <output_mlp>]\n",
    "\n",
    "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
    "    apply_log: True\n",
    "\n",
    "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
    "\n",
    "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
    "    metric: !name:speechbrain.nnet.losses.classification_error\n",
    "        reduction: batch\n",
    "\n",
    "opt_class: !name:torch.optim.Adam\n",
    "    lr: !ref <lr>\n",
    "\n",
    "ssl_opt_class: !name:torch.optim.Adam\n",
    "    lr: !ref <lr_ssl>\n",
    "\n",
    "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
    "    initial_value: !ref <lr>\n",
    "    improvement_threshold: 0.0025\n",
    "    annealing_factor: 0.9\n",
    "    patient: 0\n",
    "\n",
    "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
    "    initial_value: !ref <lr_ssl>\n",
    "    improvement_threshold: 0.0025\n",
    "    annealing_factor: 0.9\n",
    "\n",
    "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
    "    checkpoints_dir: !ref <save_folder>\n",
    "    recoverables:\n",
    "        model: !ref <model>\n",
    "        ssl_model: !ref <ssl_model>\n",
    "        lr_annealing_output: !ref <lr_annealing>\n",
    "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
    "        counter: !ref <epoch_counter>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2771b-75ca-4b65-a373-9bb375a4736d",
   "metadata": {},
   "source": [
    "### Training file\n",
    "use same trainwav2vec.py script for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f85186d-b12d-49bd-bb74-10bb65a851c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 02:16:22.472071: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-22 02:16:22.493303: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-22 02:16:22.500038: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-22 02:16:22.518756: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-22 02:16:25.885989: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "config.json: 100%|█████████████████████████| 2.24k/2.24k [00:00<00:00, 16.7MB/s]\n",
      "pytorch_model.bin: 100%|██████████████████████| 378M/378M [00:01<00:00, 353MB/s]\n",
      "preprocessor_config.json: 100%|████████████████| 215/215 [00:00<00:00, 2.30MB/s]\n",
      "speechbrain.lobes.models.huggingface_transformers.wav2vec2 - wav2vec 2.0 feature extractor is frozen.\n",
      "speechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "speechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "speechbrain.core - Beginning experiment!                                        \n",
      "speechbrain.core - Experiment folder: /home/ulaval.ca/maelr5/scratch/parkinsons-results/wavlm/base/fulldataset/1986\n",
      "model.safetensors: 100%|██████████████████████| 378M/378M [00:01<00:00, 279MB/s]\n",
      "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
      "speechbrain.core - Gradscaler enabled: `False`\n",
      "speechbrain.core - Using training precision: `--precision=fp32`\n",
      "speechbrain.core - Using evaluation precision: `--eval_precision=fp32`\n",
      "speechbrain.core - DetectorBrain Model Statistics:\n",
      "* Total Number of Trainable Parameters: 90.2M\n",
      "* Total Number of Parameters: 94.4M\n",
      "* Trainable Parameters represent 95.5496% of the total size.\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "speechbrain.utils.epoch_loop - Going into epoch 1\n",
      "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
      "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
      "  0%|                                                   | 0/163 [00:00<?, ?it/s]/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "  2%|▍                         | 3/163 [00:05<04:37,  1.73s/it, train_loss=0.84]\n",
      "speechbrain.core - Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/ssl/trainwav2vec.py\", line 379, in <module>\n",
      "    detection_brain.fit(\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/speechbrain/core.py\", line 1575, in fit\n",
      "    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/speechbrain/core.py\", line 1400, in _fit_train\n",
      "    loss = self.fit_batch(batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/speechbrain/core.py\", line 1199, in fit_batch\n",
      "    outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/ssl/trainwav2vec.py\", line 33, in compute_forward\n",
      "    outputs = self.modules.ssl_model(wavs, lens)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/speechbrain/lobes/models/huggingface_transformers/wav2vec2.py\", line 158, in forward\n",
      "    return self.extract_features(wav, wav_lens)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/speechbrain/lobes/models/huggingface_transformers/wav2vec2.py\", line 182, in extract_features\n",
      "    out = self.model(\n",
      "          ^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/transformers/models/wavlm/modeling_wavlm.py\", line 1230, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/transformers/models/wavlm/modeling_wavlm.py\", line 724, in forward\n",
      "    layer_outputs = layer(\n",
      "                    ^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/transformers/models/wavlm/modeling_wavlm.py\", line 609, in forward\n",
      "    hidden_states, attn_weights, position_bias = self.attention(\n",
      "                                                 ^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/transformers/models/wavlm/modeling_wavlm.py\", line 474, in forward\n",
      "    attn_output, attn_weights = self.torch_multi_head_self_attention(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/transformers/models/wavlm/modeling_wavlm.py\", line 498, in torch_multi_head_self_attention\n",
      "    attn_output, attn_weights = F.multi_head_attention_forward(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ulaval.ca/maelr5/parkinsons/parksenv/lib/python3.11/site-packages/torch/nn/functional.py\", line 6350, in multi_head_attention_forward\n",
      "    attn_mask = attn_mask + key_padding_mask\n",
      "                ~~~~~~~~~~^~~~~~~~~~~~~~~~~~\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.32 GiB. GPU 0 has a total capacity of 39.49 GiB of which 2.37 GiB is free. Including non-PyTorch memory, this process has 37.11 GiB memory in use. Of the allocated memory 36.01 GiB is allocated by PyTorch, and 614.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /home/ulaval.ca/maelr5/scratch/parkinsons-results/wavlm/base/fulldataset/1986\n",
    "\n",
    "import sys\n",
    "\n",
    "!{sys.executable} trainwav2vec.py hparams_wavlm.yaml --data_folder='/home/ulaval.ca/maelr5/scratch/parkinsons' --device='cuda:0' --number_of_epochs=10 --use_tensorboard=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64428a67-9cfa-435f-9330-dc62ba5cb31d",
   "metadata": {},
   "source": [
    "### output (Train log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8dc871-e113-49b9-8024-a63bf13bdeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conf_matrix = np.array([[48, 0],   # True PD\n",
    "                        [4, 34]])  # True HC\n",
    "\n",
    "plot_cm(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe4b7c-99d4-4be5-9e70-ce3332ae2f8b",
   "metadata": {},
   "source": [
    "### plot the train/valid loss/accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac24805-e50b-47b0-bc13-19d23d96d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_file = \"/home/ulaval.ca/maelr5/scratch/parkinsons-results/wav2vec2/base/fulldataset/1986/tb_logs/events.out.tfevents.1745281544.ul-val-pr-cpc02.l.ul.ca.2209524.0\"\n",
    "plot_training(event_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parksenv",
   "language": "python",
   "name": "parksenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
